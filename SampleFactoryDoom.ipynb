{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":13140,"status":"ok","timestamp":1721813495573,"user":{"displayName":"Vanipenta Naga Nithin Reddy","userId":"10356624584404995155"},"user_tz":-330},"id":"gmE4riSaE6CS","outputId":"a9a4e2b7-4e0d-4701-8ac6-9f8753b460e1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","E: Unable to locate package !nasm\n","E: Couldn't find any package by glob '!nasm'\n","E: Unable to locate package !libopenal-dev\n","E: Couldn't find any package by glob '!libopenal-dev'\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","libboost-all-dev is already the newest version (1.74.0.3ubuntu7).\n","0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","Note, selecting 'liblua5.1-0-dev' instead of 'liblua5.1-dev'\n","The following additional packages will be installed:\n","  liblua5.1-0 libtool libtool-bin\n","Suggested packages:\n","  libtool-doc gcj-jdk\n","The following NEW packages will be installed:\n","  liblua5.1-0 liblua5.1-0-dev libtool libtool-bin\n","0 upgraded, 4 newly installed, 0 to remove and 45 not upgraded.\n","Need to get 467 kB of archives.\n","After this operation, 2,859 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 liblua5.1-0 amd64 5.1.5-8.1build4 [99.9 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 liblua5.1-0-dev amd64 5.1.5-8.1build4 [122 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtool all 2.4.6-15build2 [164 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtool-bin amd64 2.4.6-15build2 [81.5 kB]\n","Fetched 467 kB in 1s (676 kB/s)\n","Selecting previously unselected package liblua5.1-0:amd64.\n","(Reading database ... 123589 files and directories currently installed.)\n","Preparing to unpack .../liblua5.1-0_5.1.5-8.1build4_amd64.deb ...\n","Unpacking liblua5.1-0:amd64 (5.1.5-8.1build4) ...\n","Selecting previously unselected package liblua5.1-0-dev:amd64.\n","Preparing to unpack .../liblua5.1-0-dev_5.1.5-8.1build4_amd64.deb ...\n","Unpacking liblua5.1-0-dev:amd64 (5.1.5-8.1build4) ...\n","Selecting previously unselected package libtool.\n","Preparing to unpack .../libtool_2.4.6-15build2_all.deb ...\n","Unpacking libtool (2.4.6-15build2) ...\n","Selecting previously unselected package libtool-bin.\n","Preparing to unpack .../libtool-bin_2.4.6-15build2_amd64.deb ...\n","Unpacking libtool-bin (2.4.6-15build2) ...\n","Setting up libtool (2.4.6-15build2) ...\n","Setting up liblua5.1-0:amd64 (5.1.5-8.1build4) ...\n","Setting up libtool-bin (2.4.6-15build2) ...\n","Setting up liblua5.1-0-dev:amd64 (5.1.5-8.1build4) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n"]}],"source":["# Install ViZDoom deps from\n","# https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md#-linux\n","\n","!apt-get install build-essential zlib1g-dev libsdl2-dev libjpeg-dev \\\n","!nasm tar libbz2-dev libgtk2.0-dev cmake git libfluidsynth-dev libgme-dev \\\n","!libopenal-dev timidity libwildmidi-dev unzip ffmpeg\n","\n","# Boost libraries\n","!apt-get install libboost-all-dev\n","\n","# Lua binding dependencies\n","!apt-get install liblua5.1-dev"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":107196,"status":"ok","timestamp":1721813602762,"user":{"displayName":"Vanipenta Naga Nithin Reddy","userId":"10356624584404995155"},"user_tz":-330},"id":"kc3MoZqGFE8y","outputId":"8ac68c01-e8e8-40ee-abf7-eaa72918f8e9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting sample-factory\n","  Downloading sample_factory-2.1.1-py3-none-any.whl (9.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy\u003c2.0,\u003e=1.18.1 in /usr/local/lib/python3.10/dist-packages (from sample-factory) (1.25.2)\n","Requirement already satisfied: torch!=1.13.0,\u003c3.0,\u003e=1.9 in /usr/local/lib/python3.10/dist-packages (from sample-factory) (2.3.1+cu121)\n","Collecting gymnasium\u003c1.0,\u003e=0.27 (from sample-factory)\n","  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyglet (from sample-factory)\n","  Downloading pyglet-2.0.16-py3-none-any.whl (929 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m929.1/929.1 kB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorboard\u003e=1.15.0 in /usr/local/lib/python3.10/dist-packages (from sample-factory) (2.15.2)\n","Collecting tensorboardx\u003e=2.0 (from sample-factory)\n","  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil\u003e=5.7.0 in /usr/local/lib/python3.10/dist-packages (from sample-factory) (5.9.5)\n","Requirement already satisfied: threadpoolctl\u003e=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sample-factory) (3.5.0)\n","Collecting colorlog (from sample-factory)\n","  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n","Collecting signal-slot-mp\u003c2.0,\u003e=1.0.3 (from sample-factory)\n","  Downloading signal_slot_mp-1.0.5-py3-none-any.whl (13 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from sample-factory) (3.15.4)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from sample-factory) (4.8.0.76)\n","Collecting wandb\u003e=0.12.9 (from sample-factory)\n","  Downloading wandb-0.17.5-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: huggingface-hub\u003c1.0,\u003e=0.10.0 in /usr/local/lib/python3.10/dist-packages (from sample-factory) (0.23.5)\n","Requirement already satisfied: cloudpickle\u003e=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium\u003c1.0,\u003e=0.27-\u003esample-factory) (2.2.1)\n","Requirement already satisfied: typing-extensions\u003e=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium\u003c1.0,\u003e=0.27-\u003esample-factory) (4.12.2)\n","Collecting farama-notifications\u003e=0.0.1 (from gymnasium\u003c1.0,\u003e=0.27-\u003esample-factory)\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Requirement already satisfied: fsspec\u003e=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.10.0-\u003esample-factory) (2023.6.0)\n","Requirement already satisfied: packaging\u003e=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.10.0-\u003esample-factory) (24.1)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.10.0-\u003esample-factory) (6.0.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.10.0-\u003esample-factory) (2.31.0)\n","Requirement already satisfied: tqdm\u003e=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.10.0-\u003esample-factory) (4.66.4)\n","Collecting faster-fifo\u003c2.0,\u003e=1.4.4 (from signal-slot-mp\u003c2.0,\u003e=1.0.3-\u003esample-factory)\n","  Downloading faster_fifo-1.4.7.tar.gz (112 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/112.6 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: absl-py\u003e=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard\u003e=1.15.0-\u003esample-factory) (1.4.0)\n","Requirement already satisfied: grpcio\u003e=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard\u003e=1.15.0-\u003esample-factory) (1.64.1)\n","Requirement already satisfied: google-auth\u003c3,\u003e=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard\u003e=1.15.0-\u003esample-factory) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib\u003c2,\u003e=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard\u003e=1.15.0-\u003esample-factory) (1.2.1)\n","Requirement already satisfied: markdown\u003e=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard\u003e=1.15.0-\u003esample-factory) (3.6)\n","Requirement already satisfied: protobuf!=4.24.0,\u003e=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard\u003e=1.15.0-\u003esample-factory) (3.20.3)\n","Requirement already satisfied: setuptools\u003e=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard\u003e=1.15.0-\u003esample-factory) (67.7.2)\n","Requirement already satisfied: six\u003e1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard\u003e=1.15.0-\u003esample-factory) (1.16.0)\n","Requirement already satisfied: tensorboard-data-server\u003c0.8.0,\u003e=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard\u003e=1.15.0-\u003esample-factory) (0.7.2)\n","Requirement already satisfied: werkzeug\u003e=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard\u003e=1.15.0-\u003esample-factory) (3.0.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.13.0,\u003c3.0,\u003e=1.9-\u003esample-factory) (1.13.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.13.0,\u003c3.0,\u003e=1.9-\u003esample-factory) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.13.0,\u003c3.0,\u003e=1.9-\u003esample-factory) (3.1.4)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch!=1.13.0,\u003c3.0,\u003e=1.9-\u003esample-factory)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch!=1.13.0,\u003c3.0,\u003e=1.9-\u003esample-factory)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch!=1.13.0,\u003c3.0,\u003e=1.9-\u003esample-factory)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch!=1.13.0,\u003c3.0,\u003e=1.9-\u003esample-factory)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch!=1.13.0,\u003c3.0,\u003e=1.9-\u003esample-factory)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch!=1.13.0,\u003c3.0,\u003e=1.9-\u003esample-factory)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch!=1.13.0,\u003c3.0,\u003e=1.9-\u003esample-factory)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch!=1.13.0,\u003c3.0,\u003e=1.9-\u003esample-factory)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch!=1.13.0,\u003c3.0,\u003e=1.9-\u003esample-factory)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch!=1.13.0,\u003c3.0,\u003e=1.9-\u003esample-factory)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch!=1.13.0,\u003c3.0,\u003e=1.9-\u003esample-factory)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch!=1.13.0,\u003c3.0,\u003e=1.9-\u003esample-factory) (2.3.1)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107-\u003etorch!=1.13.0,\u003c3.0,\u003e=1.9-\u003esample-factory)\n","  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: click!=8.0.0,\u003e=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb\u003e=0.12.9-\u003esample-factory) (8.1.7)\n","Collecting docker-pycreds\u003e=0.4.0 (from wandb\u003e=0.12.9-\u003esample-factory)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting gitpython!=3.1.29,\u003e=1.0.0 (from wandb\u003e=0.12.9-\u003esample-factory)\n","  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb\u003e=0.12.9-\u003esample-factory) (4.2.2)\n","Collecting sentry-sdk\u003e=1.0.0 (from wandb\u003e=0.12.9-\u003esample-factory)\n","  Downloading sentry_sdk-2.11.0-py2.py3-none-any.whl (303 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.6/303.6 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting setproctitle (from wandb\u003e=0.12.9-\u003esample-factory)\n","  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: cython\u003e=0.29 in /usr/local/lib/python3.10/dist-packages (from faster-fifo\u003c2.0,\u003e=1.4.4-\u003esignal-slot-mp\u003c2.0,\u003e=1.0.3-\u003esample-factory) (3.0.10)\n","Collecting gitdb\u003c5,\u003e=4.0.1 (from gitpython!=3.1.29,\u003e=1.0.0-\u003ewandb\u003e=0.12.9-\u003esample-factory)\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cachetools\u003c6.0,\u003e=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003e=1.15.0-\u003esample-factory) (5.4.0)\n","Requirement already satisfied: pyasn1-modules\u003e=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003e=1.15.0-\u003esample-factory) (0.4.0)\n","Requirement already satisfied: rsa\u003c5,\u003e=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003e=1.15.0-\u003esample-factory) (4.9)\n","Requirement already satisfied: requests-oauthlib\u003e=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib\u003c2,\u003e=0.5-\u003etensorboard\u003e=1.15.0-\u003esample-factory) (1.3.1)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ehuggingface-hub\u003c1.0,\u003e=0.10.0-\u003esample-factory) (3.3.2)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ehuggingface-hub\u003c1.0,\u003e=0.10.0-\u003esample-factory) (3.7)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ehuggingface-hub\u003c1.0,\u003e=0.10.0-\u003esample-factory) (2.0.7)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ehuggingface-hub\u003c1.0,\u003e=0.10.0-\u003esample-factory) (2024.7.4)\n","Requirement already satisfied: MarkupSafe\u003e=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug\u003e=1.0.1-\u003etensorboard\u003e=1.15.0-\u003esample-factory) (2.1.5)\n","Requirement already satisfied: mpmath\u003c1.4,\u003e=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy-\u003etorch!=1.13.0,\u003c3.0,\u003e=1.9-\u003esample-factory) (1.3.0)\n","Collecting smmap\u003c6,\u003e=3.0.1 (from gitdb\u003c5,\u003e=4.0.1-\u003egitpython!=3.1.29,\u003e=1.0.0-\u003ewandb\u003e=0.12.9-\u003esample-factory)\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Requirement already satisfied: pyasn1\u003c0.7.0,\u003e=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules\u003e=0.2.1-\u003egoogle-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003e=1.15.0-\u003esample-factory) (0.6.0)\n","Requirement already satisfied: oauthlib\u003e=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib\u003e=0.7.0-\u003egoogle-auth-oauthlib\u003c2,\u003e=0.5-\u003etensorboard\u003e=1.15.0-\u003esample-factory) (3.2.2)\n","Building wheels for collected packages: faster-fifo\n","  Building wheel for faster-fifo (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for faster-fifo: filename=faster_fifo-1.4.7-cp310-cp310-linux_x86_64.whl size=355047 sha256=4770825f6882949a2b23d1ddaddcf8b6bb33bd493f8f3d217332903cbaec25b0\n","  Stored in directory: /root/.cache/pip/wheels/bc/66/d1/e4d2eb7b7a15fb57371c07f8298acfa1b528d05618f5e1d91d\n","Successfully built faster-fifo\n","Installing collected packages: farama-notifications, tensorboardx, smmap, setproctitle, sentry-sdk, pyglet, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, gymnasium, faster-fifo, docker-pycreds, colorlog, signal-slot-mp, nvidia-cusparse-cu12, nvidia-cudnn-cu12, gitdb, nvidia-cusolver-cu12, gitpython, wandb, sample-factory\n","Successfully installed colorlog-6.8.2 docker-pycreds-0.4.0 farama-notifications-0.0.4 faster-fifo-1.4.7 gitdb-4.0.11 gitpython-3.1.43 gymnasium-0.29.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 pyglet-2.0.16 sample-factory-2.1.1 sentry-sdk-2.11.0 setproctitle-1.3.3 signal-slot-mp-1.0.5 smmap-5.0.1 tensorboardx-2.6.2.2 wandb-0.17.5\n","Collecting vizdoom\n","  Downloading vizdoom-1.2.3-cp310-cp310-manylinux_2_28_x86_64.whl (28.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.0/28.0 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from vizdoom) (1.25.2)\n","Requirement already satisfied: gymnasium\u003e=0.28.0 in /usr/local/lib/python3.10/dist-packages (from vizdoom) (0.29.1)\n","Requirement already satisfied: pygame\u003e=2.1.3 in /usr/local/lib/python3.10/dist-packages (from vizdoom) (2.6.0)\n","Requirement already satisfied: cloudpickle\u003e=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium\u003e=0.28.0-\u003evizdoom) (2.2.1)\n","Requirement already satisfied: typing-extensions\u003e=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium\u003e=0.28.0-\u003evizdoom) (4.12.2)\n","Requirement already satisfied: farama-notifications\u003e=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium\u003e=0.28.0-\u003evizdoom) (0.0.4)\n","Installing collected packages: vizdoom\n","Successfully installed vizdoom-1.2.3\n"]}],"source":["!pip install sample-factory\n","!pip install vizdoom"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":5457,"status":"ok","timestamp":1721813608208,"user":{"displayName":"Vanipenta Naga Nithin Reddy","userId":"10356624584404995155"},"user_tz":-330},"id":"3EScVHSGFGHr"},"outputs":[],"source":["import functools\n","\n","from sample_factory.algo.utils.context import global_model_factory\n","from sample_factory.cfg.arguments import parse_full_cfg, parse_sf_args\n","from sample_factory.envs.env_utils import register_env\n","from sample_factory.train import run_rl\n","\n","from sf_examples.vizdoom.doom.doom_model import make_vizdoom_encoder\n","from sf_examples.vizdoom.doom.doom_params import add_doom_env_args, doom_override_defaults\n","from sf_examples.vizdoom.doom.doom_utils import DOOM_ENVS, make_doom_env_from_spec\n","\n","\n","# Registers all the ViZDoom environments\n","def register_vizdoom_envs():\n","    for env_spec in DOOM_ENVS:\n","        make_env_func = functools.partial(make_doom_env_from_spec, env_spec)\n","        register_env(env_spec.name, make_env_func)\n","\n","\n","# Sample Factory allows the registration of a custom Neural Network architecture\n","# See https://github.com/alex-petrenko/sample-factory/blob/master/sf_examples/vizdoom/doom/doom_model.py for more details\n","def register_vizdoom_models():\n","    global_model_factory().register_encoder_factory(make_vizdoom_encoder)\n","\n","\n","def register_vizdoom_components():\n","    register_vizdoom_envs()\n","    register_vizdoom_models()\n","\n","\n","# parse the command line args and create a config\n","def parse_vizdoom_cfg(argv=None, evaluation=False):\n","    parser, _ = parse_sf_args(argv=argv, evaluation=evaluation)\n","    # parameters specific to Doom envs\n","    add_doom_env_args(parser)\n","    # override Doom default values for algo parameters\n","    doom_override_defaults(parser)\n","    # second parsing pass yields the final configuration\n","    final_cfg = parse_full_cfg(parser, argv)\n","    return final_cfg"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1166567,"status":"ok","timestamp":1721814774759,"user":{"displayName":"Vanipenta Naga Nithin Reddy","userId":"10356624584404995155"},"user_tz":-330},"id":"ooE2KueoFHM1","outputId":"bd962196-4da3-4a82-cedd-289748c0152d"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[36m[2024-07-24 09:33:24,357][00731] register_encoder_factory: \u003cfunction make_vizdoom_encoder at 0x7decd2c02050\u003e\u001b[0m\n","\u001b[33m[2024-07-24 09:33:24,391][00731] Saved parameter configuration for experiment default_experiment not found!\u001b[0m\n","\u001b[33m[2024-07-24 09:33:24,392][00731] Starting experiment from scratch!\u001b[0m\n","\u001b[36m[2024-07-24 09:33:24,412][00731] Experiment dir /content/train_dir/default_experiment already exists!\u001b[0m\n","\u001b[36m[2024-07-24 09:33:24,420][00731] Resuming existing experiment from /content/train_dir/default_experiment...\u001b[0m\n","\u001b[36m[2024-07-24 09:33:24,422][00731] Weights and Biases integration disabled\u001b[0m\n","\u001b[36m[2024-07-24 09:33:26,964][00731] Queried available GPUs: 0\n","\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:33:26,966][00731] Environment var CUDA_VISIBLE_DEVICES is 0\n","\u001b[0m\n","\u001b[36m[2024-07-24 09:33:29,394][00731] Automatically setting recurrence to 32\u001b[0m\n","\u001b[36m[2024-07-24 09:33:29,397][00731] Starting experiment with the following configuration:\n","help=False\n","algo=APPO\n","env=doom_health_gathering_supreme\n","experiment=default_experiment\n","train_dir=/content/train_dir\n","restart_behavior=resume\n","device=gpu\n","seed=None\n","num_policies=1\n","async_rl=True\n","serial_mode=False\n","batched_sampling=False\n","num_batches_to_accumulate=2\n","worker_num_splits=2\n","policy_workers_per_policy=1\n","max_policy_lag=1000\n","num_workers=8\n","num_envs_per_worker=4\n","batch_size=1024\n","num_batches_per_epoch=1\n","num_epochs=1\n","rollout=32\n","recurrence=32\n","shuffle_minibatches=False\n","gamma=0.99\n","reward_scale=1.0\n","reward_clip=1000.0\n","value_bootstrap=False\n","normalize_returns=True\n","exploration_loss_coeff=0.001\n","value_loss_coeff=0.5\n","kl_loss_coeff=0.0\n","exploration_loss=symmetric_kl\n","gae_lambda=0.95\n","ppo_clip_ratio=0.1\n","ppo_clip_value=0.2\n","with_vtrace=False\n","vtrace_rho=1.0\n","vtrace_c=1.0\n","optimizer=adam\n","adam_eps=1e-06\n","adam_beta1=0.9\n","adam_beta2=0.999\n","max_grad_norm=4.0\n","learning_rate=0.0001\n","lr_schedule=constant\n","lr_schedule_kl_threshold=0.008\n","lr_adaptive_min=1e-06\n","lr_adaptive_max=0.01\n","obs_subtract_mean=0.0\n","obs_scale=255.0\n","normalize_input=True\n","normalize_input_keys=None\n","decorrelate_experience_max_seconds=0\n","decorrelate_envs_on_one_worker=True\n","actor_worker_gpus=[]\n","set_workers_cpu_affinity=True\n","force_envs_single_thread=False\n","default_niceness=0\n","log_to_file=True\n","experiment_summaries_interval=10\n","flush_summaries_interval=30\n","stats_avg=100\n","summaries_use_frameskip=True\n","heartbeat_interval=20\n","heartbeat_reporting_interval=600\n","train_for_env_steps=4000000\n","train_for_seconds=10000000000\n","save_every_sec=120\n","keep_checkpoints=2\n","load_checkpoint_kind=latest\n","save_milestones_sec=-1\n","save_best_every_sec=5\n","save_best_metric=reward\n","save_best_after=100000\n","benchmark=False\n","encoder_mlp_layers=[512, 512]\n","encoder_conv_architecture=convnet_simple\n","encoder_conv_mlp_layers=[512]\n","use_rnn=True\n","rnn_size=512\n","rnn_type=gru\n","rnn_num_layers=1\n","decoder_mlp_layers=[]\n","nonlinearity=elu\n","policy_initialization=orthogonal\n","policy_init_gain=1.0\n","actor_critic_share_weights=True\n","adaptive_stddev=True\n","continuous_tanh_scale=0.0\n","initial_stddev=1.0\n","use_env_info_cache=False\n","env_gpu_actions=False\n","env_gpu_observations=True\n","env_frameskip=4\n","env_framestack=1\n","pixel_format=CHW\n","use_record_episode_statistics=False\n","with_wandb=False\n","wandb_user=None\n","wandb_project=sample_factory\n","wandb_group=None\n","wandb_job_type=SF\n","wandb_tags=[]\n","with_pbt=False\n","pbt_mix_policies_in_one_env=True\n","pbt_period_env_steps=5000000\n","pbt_start_mutation=20000000\n","pbt_replace_fraction=0.3\n","pbt_mutation_rate=0.15\n","pbt_replace_reward_gap=0.1\n","pbt_replace_reward_gap_absolute=1e-06\n","pbt_optimize_gamma=False\n","pbt_target_objective=true_objective\n","pbt_perturb_min=1.1\n","pbt_perturb_max=1.5\n","num_agents=-1\n","num_humans=0\n","num_bots=-1\n","start_bot_difficulty=None\n","timelimit=None\n","res_w=128\n","res_h=72\n","wide_aspect_ratio=False\n","eval_env_frameskip=1\n","fps=35\n","command_line=--env=doom_health_gathering_supreme --num_workers=8 --num_envs_per_worker=4 --train_for_env_steps=4000000\n","cli_args={'env': 'doom_health_gathering_supreme', 'num_workers': 8, 'num_envs_per_worker': 4, 'train_for_env_steps': 4000000}\n","git_hash=unknown\n","git_repo_name=not a git repository\u001b[0m\n","\u001b[36m[2024-07-24 09:33:29,398][00731] Saving configuration to /content/train_dir/default_experiment/config.json...\u001b[0m\n","\u001b[36m[2024-07-24 09:33:29,401][00731] Rollout worker 0 uses device cpu\u001b[0m\n","\u001b[36m[2024-07-24 09:33:29,404][00731] Rollout worker 1 uses device cpu\u001b[0m\n","\u001b[36m[2024-07-24 09:33:29,405][00731] Rollout worker 2 uses device cpu\u001b[0m\n","\u001b[36m[2024-07-24 09:33:29,407][00731] Rollout worker 3 uses device cpu\u001b[0m\n","\u001b[36m[2024-07-24 09:33:29,411][00731] Rollout worker 4 uses device cpu\u001b[0m\n","\u001b[36m[2024-07-24 09:33:29,412][00731] Rollout worker 5 uses device cpu\u001b[0m\n","\u001b[36m[2024-07-24 09:33:29,413][00731] Rollout worker 6 uses device cpu\u001b[0m\n","\u001b[36m[2024-07-24 09:33:29,414][00731] Rollout worker 7 uses device cpu\u001b[0m\n","\u001b[36m[2024-07-24 09:33:29,672][00731] Using GPUs [0] for process 0 (actually maps to GPUs [0])\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:33:29,677][00731] InferenceWorker_p0-w0: min num requests: 2\u001b[0m\n","\u001b[36m[2024-07-24 09:33:29,754][00731] Starting all processes...\u001b[0m\n","\u001b[36m[2024-07-24 09:33:29,759][00731] Starting process learner_proc0\u001b[0m\n","\u001b[36m[2024-07-24 09:33:32,650][00731] Starting all processes...\u001b[0m\n","\u001b[36m[2024-07-24 09:33:32,737][00731] Starting process inference_proc0-0\u001b[0m\n","\u001b[36m[2024-07-24 09:33:32,738][00731] Starting process rollout_proc0\u001b[0m\n","\u001b[36m[2024-07-24 09:33:32,740][00731] Starting process rollout_proc1\u001b[0m\n","\u001b[36m[2024-07-24 09:33:32,740][00731] Starting process rollout_proc2\u001b[0m\n","\u001b[36m[2024-07-24 09:33:32,740][00731] Starting process rollout_proc3\u001b[0m\n","\u001b[36m[2024-07-24 09:33:32,740][00731] Starting process rollout_proc4\u001b[0m\n","\u001b[36m[2024-07-24 09:33:32,740][00731] Starting process rollout_proc5\u001b[0m\n","\u001b[36m[2024-07-24 09:33:32,740][00731] Starting process rollout_proc6\u001b[0m\n","\u001b[36m[2024-07-24 09:33:32,742][00731] Starting process rollout_proc7\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:33:49,625][00731] Heartbeat connected on Batcher_0\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:33:50,632][00731] Heartbeat connected on RolloutWorker_w0\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:33:50,770][00731] Heartbeat connected on RolloutWorker_w4\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:33:50,902][00731] Heartbeat connected on RolloutWorker_w3\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:33:50,996][00731] Heartbeat connected on RolloutWorker_w6\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:33:51,174][00731] Heartbeat connected on RolloutWorker_w5\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:33:51,207][00731] Heartbeat connected on RolloutWorker_w1\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:33:51,273][00731] Heartbeat connected on RolloutWorker_w2\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:33:51,320][00731] Heartbeat connected on RolloutWorker_w7\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:33:51,394][00731] Heartbeat connected on InferenceWorker_p0-w0\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:33:52,368][00731] Heartbeat connected on LearnerWorker_p0\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:33:52,709][00731] Inference worker 0-0 is ready!\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:33:52,710][00731] All inference workers are ready! Signal rollout workers to start!\u001b[0m\n","\u001b[36m[2024-07-24 09:33:54,437][00731] Fps is (10 sec: nan, 60 sec: nan, 300 sec: nan). Total num frames: 0. Throughput: 0: nan. Samples: 0. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:33:59,441][00731] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 0.0. Samples: 0. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:34:04,437][00731] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 1.6. Samples: 16. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:34:04,441][00731] Avg episode reward: [(0, '1.716')]\u001b[0m\n","\u001b[36m[2024-07-24 09:34:09,437][00731] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 193.3. Samples: 2900. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:34:09,442][00731] Avg episode reward: [(0, '2.750')]\u001b[0m\n","\u001b[36m[2024-07-24 09:34:14,437][00731] Fps is (10 sec: 2048.0, 60 sec: 1024.0, 300 sec: 1024.0). Total num frames: 20480. Throughput: 0: 291.7. Samples: 5834. Policy #0 lag: (min: 0.0, avg: 0.4, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:34:14,441][00731] Avg episode reward: [(0, '3.320')]\u001b[0m\n","\u001b[36m[2024-07-24 09:34:19,437][00731] Fps is (10 sec: 3276.8, 60 sec: 1310.7, 300 sec: 1310.7). Total num frames: 32768. Throughput: 0: 311.4. Samples: 7784. Policy #0 lag: (min: 0.0, avg: 0.4, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:34:19,443][00731] Avg episode reward: [(0, '3.660')]\u001b[0m\n","\u001b[36m[2024-07-24 09:34:24,437][00731] Fps is (10 sec: 3686.4, 60 sec: 1911.5, 300 sec: 1911.5). Total num frames: 57344. Throughput: 0: 434.2. Samples: 13026. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:34:24,443][00731] Avg episode reward: [(0, '4.310')]\u001b[0m\n","\u001b[36m[2024-07-24 09:34:29,437][00731] Fps is (10 sec: 4505.6, 60 sec: 2223.5, 300 sec: 2223.5). Total num frames: 77824. Throughput: 0: 556.1. Samples: 19462. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:34:29,443][00731] Avg episode reward: [(0, '4.321')]\u001b[0m\n","\u001b[36m[2024-07-24 09:34:34,438][00731] Fps is (10 sec: 3276.3, 60 sec: 2252.7, 300 sec: 2252.7). Total num frames: 90112. Throughput: 0: 540.9. Samples: 21638. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:34:34,442][00731] Avg episode reward: [(0, '4.401')]\u001b[0m\n","\u001b[36m[2024-07-24 09:34:39,437][00731] Fps is (10 sec: 2867.2, 60 sec: 2366.6, 300 sec: 2366.6). Total num frames: 106496. Throughput: 0: 578.7. Samples: 26042. Policy #0 lag: (min: 0.0, avg: 0.5, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:34:39,443][00731] Avg episode reward: [(0, '4.247')]\u001b[0m\n","\u001b[36m[2024-07-24 09:34:44,437][00731] Fps is (10 sec: 3687.0, 60 sec: 2539.5, 300 sec: 2539.5). Total num frames: 126976. Throughput: 0: 716.8. Samples: 32254. Policy #0 lag: (min: 0.0, avg: 0.4, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:34:44,439][00731] Avg episode reward: [(0, '4.199')]\u001b[0m\n","\u001b[36m[2024-07-24 09:34:49,437][00731] Fps is (10 sec: 4096.0, 60 sec: 2681.0, 300 sec: 2681.0). Total num frames: 147456. Throughput: 0: 788.9. Samples: 35518. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:34:49,439][00731] Avg episode reward: [(0, '4.377')]\u001b[0m\n","\u001b[36m[2024-07-24 09:34:54,437][00731] Fps is (10 sec: 3276.8, 60 sec: 2662.4, 300 sec: 2662.4). Total num frames: 159744. Throughput: 0: 812.7. Samples: 39470. Policy #0 lag: (min: 0.0, avg: 0.4, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:34:54,439][00731] Avg episode reward: [(0, '4.439')]\u001b[0m\n","\u001b[36m[2024-07-24 09:34:59,437][00731] Fps is (10 sec: 3276.7, 60 sec: 3003.9, 300 sec: 2772.7). Total num frames: 180224. Throughput: 0: 880.2. Samples: 45444. Policy #0 lag: (min: 0.0, avg: 0.5, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:34:59,440][00731] Avg episode reward: [(0, '4.468')]\u001b[0m\n","\u001b[36m[2024-07-24 09:35:04,437][00731] Fps is (10 sec: 4096.0, 60 sec: 3345.1, 300 sec: 2867.2). Total num frames: 200704. Throughput: 0: 906.9. Samples: 48596. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:35:04,439][00731] Avg episode reward: [(0, '4.342')]\u001b[0m\n","\u001b[36m[2024-07-24 09:35:09,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3549.9, 300 sec: 2839.9). Total num frames: 212992. Throughput: 0: 899.8. Samples: 53516. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:35:09,439][00731] Avg episode reward: [(0, '4.336')]\u001b[0m\n","\u001b[36m[2024-07-24 09:35:14,437][00731] Fps is (10 sec: 2867.2, 60 sec: 3481.6, 300 sec: 2867.2). Total num frames: 229376. Throughput: 0: 863.3. Samples: 58310. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:35:14,443][00731] Avg episode reward: [(0, '4.307')]\u001b[0m\n","\u001b[36m[2024-07-24 09:35:19,437][00731] Fps is (10 sec: 4096.0, 60 sec: 3686.4, 300 sec: 2987.7). Total num frames: 253952. Throughput: 0: 887.3. Samples: 61564. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:35:19,441][00731] Avg episode reward: [(0, '4.446')]\u001b[0m\n","\u001b[36m[2024-07-24 09:35:24,437][00731] Fps is (10 sec: 4096.0, 60 sec: 3549.9, 300 sec: 3003.7). Total num frames: 270336. Throughput: 0: 923.4. Samples: 67596. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:35:24,442][00731] Avg episode reward: [(0, '4.565')]\u001b[0m\n","\u001b[36m[2024-07-24 09:35:29,437][00731] Fps is (10 sec: 2867.2, 60 sec: 3413.3, 300 sec: 2975.0). Total num frames: 282624. Throughput: 0: 871.1. Samples: 71452. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:35:29,443][00731] Avg episode reward: [(0, '4.469')]\u001b[0m\n","\u001b[36m[2024-07-24 09:35:34,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3549.9, 300 sec: 3031.0). Total num frames: 303104. Throughput: 0: 857.0. Samples: 74084. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:35:34,444][00731] Avg episode reward: [(0, '4.437')]\u001b[0m\n","\u001b[36m[2024-07-24 09:35:39,437][00731] Fps is (10 sec: 4095.8, 60 sec: 3618.1, 300 sec: 3081.7). Total num frames: 323584. Throughput: 0: 911.3. Samples: 80478. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:35:39,440][00731] Avg episode reward: [(0, '4.415')]\u001b[0m\n","\u001b[36m[2024-07-24 09:35:44,437][00731] Fps is (10 sec: 3686.2, 60 sec: 3549.8, 300 sec: 3090.6). Total num frames: 339968. Throughput: 0: 881.9. Samples: 85130. Policy #0 lag: (min: 0.0, avg: 0.5, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:35:44,443][00731] Avg episode reward: [(0, '4.430')]\u001b[0m\n","\u001b[36m[2024-07-24 09:35:49,437][00731] Fps is (10 sec: 3277.0, 60 sec: 3481.6, 300 sec: 3098.7). Total num frames: 356352. Throughput: 0: 858.1. Samples: 87210. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:35:49,444][00731] Avg episode reward: [(0, '4.313')]\u001b[0m\n","\u001b[36m[2024-07-24 09:35:54,437][00731] Fps is (10 sec: 3686.6, 60 sec: 3618.1, 300 sec: 3140.3). Total num frames: 376832. Throughput: 0: 889.4. Samples: 93538. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:35:54,445][00731] Avg episode reward: [(0, '4.518')]\u001b[0m\n","\u001b[36m[2024-07-24 09:35:59,437][00731] Fps is (10 sec: 3686.4, 60 sec: 3549.9, 300 sec: 3145.7). Total num frames: 393216. Throughput: 0: 910.1. Samples: 99266. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:35:59,441][00731] Avg episode reward: [(0, '4.729')]\u001b[0m\n","\u001b[36m[2024-07-24 09:36:04,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3481.6, 300 sec: 3150.8). Total num frames: 409600. Throughput: 0: 879.7. Samples: 101152. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:36:04,443][00731] Avg episode reward: [(0, '4.473')]\u001b[0m\n","\u001b[36m[2024-07-24 09:36:09,437][00731] Fps is (10 sec: 3686.4, 60 sec: 3618.1, 300 sec: 3185.8). Total num frames: 430080. Throughput: 0: 865.7. Samples: 106554. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:36:09,444][00731] Avg episode reward: [(0, '4.493')]\u001b[0m\n","\u001b[36m[2024-07-24 09:36:14,437][00731] Fps is (10 sec: 4096.0, 60 sec: 3686.4, 300 sec: 3218.3). Total num frames: 450560. Throughput: 0: 922.4. Samples: 112962. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:36:14,442][00731] Avg episode reward: [(0, '4.595')]\u001b[0m\n","\u001b[36m[2024-07-24 09:36:19,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3481.6, 300 sec: 3192.1). Total num frames: 462848. Throughput: 0: 913.4. Samples: 115188. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:36:19,441][00731] Avg episode reward: [(0, '4.495')]\u001b[0m\n","\u001b[36m[2024-07-24 09:36:24,437][00731] Fps is (10 sec: 2867.2, 60 sec: 3481.6, 300 sec: 3194.9). Total num frames: 479232. Throughput: 0: 868.9. Samples: 119576. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:36:24,441][00731] Avg episode reward: [(0, '4.428')]\u001b[0m\n","\u001b[36m[2024-07-24 09:36:29,437][00731] Fps is (10 sec: 4096.0, 60 sec: 3686.4, 300 sec: 3250.4). Total num frames: 503808. Throughput: 0: 910.1. Samples: 126086. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:36:29,442][00731] Avg episode reward: [(0, '4.536')]\u001b[0m\n","\u001b[36m[2024-07-24 09:36:34,437][00731] Fps is (10 sec: 4095.9, 60 sec: 3618.1, 300 sec: 3251.2). Total num frames: 520192. Throughput: 0: 936.0. Samples: 129332. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:36:34,440][00731] Avg episode reward: [(0, '4.490')]\u001b[0m\n","\u001b[36m[2024-07-24 09:36:39,437][00731] Fps is (10 sec: 2867.1, 60 sec: 3481.6, 300 sec: 3227.1). Total num frames: 532480. Throughput: 0: 884.0. Samples: 133318. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:36:39,442][00731] Avg episode reward: [(0, '4.599')]\u001b[0m\n","\u001b[36m[2024-07-24 09:36:44,437][00731] Fps is (10 sec: 3686.5, 60 sec: 3618.2, 300 sec: 3276.8). Total num frames: 557056. Throughput: 0: 887.4. Samples: 139200. Policy #0 lag: (min: 0.0, avg: 0.5, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:36:44,441][00731] Avg episode reward: [(0, '4.395')]\u001b[0m\n","\u001b[36m[2024-07-24 09:36:49,437][00731] Fps is (10 sec: 4505.7, 60 sec: 3686.4, 300 sec: 3300.2). Total num frames: 577536. Throughput: 0: 917.4. Samples: 142436. Policy #0 lag: (min: 0.0, avg: 0.4, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:36:49,439][00731] Avg episode reward: [(0, '4.312')]\u001b[0m\n","\u001b[36m[2024-07-24 09:36:54,437][00731] Fps is (10 sec: 3276.6, 60 sec: 3549.8, 300 sec: 3276.8). Total num frames: 589824. Throughput: 0: 904.8. Samples: 147270. Policy #0 lag: (min: 0.0, avg: 0.6, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:36:54,443][00731] Avg episode reward: [(0, '4.311')]\u001b[0m\n","\u001b[36m[2024-07-24 09:36:59,437][00731] Fps is (10 sec: 2867.2, 60 sec: 3549.9, 300 sec: 3276.8). Total num frames: 606208. Throughput: 0: 876.0. Samples: 152382. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:36:59,439][00731] Avg episode reward: [(0, '4.338')]\u001b[0m\n","\u001b[36m[2024-07-24 09:37:04,437][00731] Fps is (10 sec: 4096.2, 60 sec: 3686.4, 300 sec: 3319.9). Total num frames: 630784. Throughput: 0: 898.9. Samples: 155640. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:37:04,439][00731] Avg episode reward: [(0, '4.462')]\u001b[0m\n","\u001b[36m[2024-07-24 09:37:09,440][00731] Fps is (10 sec: 4094.5, 60 sec: 3617.9, 300 sec: 3318.8). Total num frames: 647168. Throughput: 0: 928.3. Samples: 161352. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:37:09,445][00731] Avg episode reward: [(0, '4.607')]\u001b[0m\n","\u001b[36m[2024-07-24 09:37:14,437][00731] Fps is (10 sec: 2867.2, 60 sec: 3481.6, 300 sec: 3297.3). Total num frames: 659456. Throughput: 0: 874.7. Samples: 165448. Policy #0 lag: (min: 0.0, avg: 0.5, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:37:14,443][00731] Avg episode reward: [(0, '4.626')]\u001b[0m\n","\u001b[36m[2024-07-24 09:37:19,439][00731] Fps is (10 sec: 3277.1, 60 sec: 3618.0, 300 sec: 3316.7). Total num frames: 679936. Throughput: 0: 872.9. Samples: 168614. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:37:19,441][00731] Avg episode reward: [(0, '4.500')]\u001b[0m\n","\u001b[36m[2024-07-24 09:37:24,437][00731] Fps is (10 sec: 4096.0, 60 sec: 3686.4, 300 sec: 3335.3). Total num frames: 700416. Throughput: 0: 924.7. Samples: 174928. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:37:24,445][00731] Avg episode reward: [(0, '4.562')]\u001b[0m\n","\u001b[36m[2024-07-24 09:37:29,438][00731] Fps is (10 sec: 3686.8, 60 sec: 3549.8, 300 sec: 3333.9). Total num frames: 716800. Throughput: 0: 891.4. Samples: 179312. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:37:29,440][00731] Avg episode reward: [(0, '4.693')]\u001b[0m\n","\u001b[36m[2024-07-24 09:37:34,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3549.9, 300 sec: 3332.7). Total num frames: 733184. Throughput: 0: 870.8. Samples: 181620. Policy #0 lag: (min: 0.0, avg: 0.5, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:37:34,442][00731] Avg episode reward: [(0, '4.683')]\u001b[0m\n","\u001b[36m[2024-07-24 09:37:39,437][00731] Fps is (10 sec: 3686.9, 60 sec: 3686.4, 300 sec: 3349.6). Total num frames: 753664. Throughput: 0: 905.4. Samples: 188014. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:37:39,441][00731] Avg episode reward: [(0, '4.655')]\u001b[0m\n","\u001b[36m[2024-07-24 09:37:44,437][00731] Fps is (10 sec: 3686.4, 60 sec: 3549.9, 300 sec: 3348.0). Total num frames: 770048. Throughput: 0: 913.2. Samples: 193474. Policy #0 lag: (min: 0.0, avg: 0.4, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:37:44,443][00731] Avg episode reward: [(0, '4.765')]\u001b[0m\n","\u001b[36m[2024-07-24 09:37:49,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3481.6, 300 sec: 3346.5). Total num frames: 786432. Throughput: 0: 884.0. Samples: 195418. Policy #0 lag: (min: 0.0, avg: 0.5, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:37:49,439][00731] Avg episode reward: [(0, '4.792')]\u001b[0m\n","\u001b[36m[2024-07-24 09:37:54,437][00731] Fps is (10 sec: 3686.3, 60 sec: 3618.2, 300 sec: 3362.1). Total num frames: 806912. Throughput: 0: 879.5. Samples: 200926. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:37:54,444][00731] Avg episode reward: [(0, '4.746')]\u001b[0m\n","\u001b[36m[2024-07-24 09:37:59,437][00731] Fps is (10 sec: 4096.0, 60 sec: 3686.4, 300 sec: 3377.1). Total num frames: 827392. Throughput: 0: 932.8. Samples: 207426. Policy #0 lag: (min: 0.0, avg: 0.4, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:37:59,442][00731] Avg episode reward: [(0, '4.697')]\u001b[0m\n","\u001b[36m[2024-07-24 09:38:04,437][00731] Fps is (10 sec: 3276.9, 60 sec: 3481.6, 300 sec: 3358.7). Total num frames: 839680. Throughput: 0: 907.3. Samples: 209442. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:38:04,443][00731] Avg episode reward: [(0, '4.471')]\u001b[0m\n","\u001b[36m[2024-07-24 09:38:09,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3550.1, 300 sec: 3373.2). Total num frames: 860160. Throughput: 0: 867.3. Samples: 213958. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:38:09,444][00731] Avg episode reward: [(0, '4.894')]\u001b[0m\n","\u001b[36m[2024-07-24 09:38:14,437][00731] Fps is (10 sec: 4096.0, 60 sec: 3686.4, 300 sec: 3387.1). Total num frames: 880640. Throughput: 0: 909.9. Samples: 220258. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:38:14,448][00731] Avg episode reward: [(0, '5.059')]\u001b[0m\n","\u001b[36m[2024-07-24 09:38:19,437][00731] Fps is (10 sec: 3686.2, 60 sec: 3618.3, 300 sec: 3385.0). Total num frames: 897024. Throughput: 0: 922.6. Samples: 223138. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:38:19,443][00731] Avg episode reward: [(0, '4.842')]\u001b[0m\n","\u001b[36m[2024-07-24 09:38:24,437][00731] Fps is (10 sec: 2867.2, 60 sec: 3481.6, 300 sec: 3367.8). Total num frames: 909312. Throughput: 0: 870.3. Samples: 227178. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:38:24,443][00731] Avg episode reward: [(0, '4.587')]\u001b[0m\n","\u001b[36m[2024-07-24 09:38:29,437][00731] Fps is (10 sec: 3686.6, 60 sec: 3618.2, 300 sec: 3396.0). Total num frames: 933888. Throughput: 0: 886.4. Samples: 233364. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:38:29,443][00731] Avg episode reward: [(0, '4.997')]\u001b[0m\n","\u001b[36m[2024-07-24 09:38:34,437][00731] Fps is (10 sec: 4505.6, 60 sec: 3686.4, 300 sec: 3408.5). Total num frames: 954368. Throughput: 0: 916.7. Samples: 236670. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:38:34,444][00731] Avg episode reward: [(0, '4.704')]\u001b[0m\n","\u001b[36m[2024-07-24 09:38:39,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3549.9, 300 sec: 3391.8). Total num frames: 966656. Throughput: 0: 901.5. Samples: 241494. Policy #0 lag: (min: 0.0, avg: 0.5, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:38:39,440][00731] Avg episode reward: [(0, '4.501')]\u001b[0m\n","\u001b[36m[2024-07-24 09:38:44,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3618.1, 300 sec: 3403.9). Total num frames: 987136. Throughput: 0: 871.7. Samples: 246652. Policy #0 lag: (min: 0.0, avg: 0.6, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:38:44,442][00731] Avg episode reward: [(0, '4.758')]\u001b[0m\n","\u001b[36m[2024-07-24 09:38:49,437][00731] Fps is (10 sec: 4096.0, 60 sec: 3686.4, 300 sec: 3415.6). Total num frames: 1007616. Throughput: 0: 899.3. Samples: 249912. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:38:49,439][00731] Avg episode reward: [(0, '4.863')]\u001b[0m\n","\u001b[36m[2024-07-24 09:38:54,439][00731] Fps is (10 sec: 3685.7, 60 sec: 3618.0, 300 sec: 3471.2). Total num frames: 1024000. Throughput: 0: 927.0. Samples: 255674. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:38:54,447][00731] Avg episode reward: [(0, '4.913')]\u001b[0m\n","\u001b[36m[2024-07-24 09:38:59,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3549.9, 300 sec: 3526.7). Total num frames: 1040384. Throughput: 0: 881.0. Samples: 259902. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:38:59,444][00731] Avg episode reward: [(0, '4.880')]\u001b[0m\n","\u001b[36m[2024-07-24 09:39:04,437][00731] Fps is (10 sec: 3687.1, 60 sec: 3686.4, 300 sec: 3596.1). Total num frames: 1060864. Throughput: 0: 888.4. Samples: 263116. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:39:04,444][00731] Avg episode reward: [(0, '5.201')]\u001b[0m\n","\u001b[36m[2024-07-24 09:39:09,437][00731] Fps is (10 sec: 4096.0, 60 sec: 3686.4, 300 sec: 3596.2). Total num frames: 1081344. Throughput: 0: 943.3. Samples: 269628. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:39:09,439][00731] Avg episode reward: [(0, '5.359')]\u001b[0m\n","\u001b[36m[2024-07-24 09:39:14,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3549.9, 300 sec: 3596.1). Total num frames: 1093632. Throughput: 0: 897.6. Samples: 273758. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:39:14,439][00731] Avg episode reward: [(0, '5.179')]\u001b[0m\n","\u001b[36m[2024-07-24 09:39:19,437][00731] Fps is (10 sec: 2867.2, 60 sec: 3549.9, 300 sec: 3568.4). Total num frames: 1110016. Throughput: 0: 878.0. Samples: 276182. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:39:19,443][00731] Avg episode reward: [(0, '5.168')]\u001b[0m\n","\u001b[36m[2024-07-24 09:39:24,437][00731] Fps is (10 sec: 4096.0, 60 sec: 3754.7, 300 sec: 3582.3). Total num frames: 1134592. Throughput: 0: 919.7. Samples: 282882. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:39:24,443][00731] Avg episode reward: [(0, '5.337')]\u001b[0m\n","\u001b[36m[2024-07-24 09:39:29,440][00731] Fps is (10 sec: 4094.6, 60 sec: 3617.9, 300 sec: 3596.1). Total num frames: 1150976. Throughput: 0: 921.3. Samples: 288112. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:39:29,443][00731] Avg episode reward: [(0, '5.374')]\u001b[0m\n","\u001b[36m[2024-07-24 09:39:34,437][00731] Fps is (10 sec: 2867.2, 60 sec: 3481.6, 300 sec: 3582.3). Total num frames: 1163264. Throughput: 0: 892.7. Samples: 290084. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:39:34,439][00731] Avg episode reward: [(0, '5.534')]\u001b[0m\n","\u001b[36m[2024-07-24 09:39:39,437][00731] Fps is (10 sec: 3687.7, 60 sec: 3686.4, 300 sec: 3596.1). Total num frames: 1187840. Throughput: 0: 898.3. Samples: 296094. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:39:39,439][00731] Avg episode reward: [(0, '5.605')]\u001b[0m\n","\u001b[36m[2024-07-24 09:39:44,438][00731] Fps is (10 sec: 4504.9, 60 sec: 3686.3, 300 sec: 3596.1). Total num frames: 1208320. Throughput: 0: 942.6. Samples: 302322. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:39:44,441][00731] Avg episode reward: [(0, '5.607')]\u001b[0m\n","\u001b[36m[2024-07-24 09:39:49,437][00731] Fps is (10 sec: 3276.7, 60 sec: 3549.8, 300 sec: 3596.1). Total num frames: 1220608. Throughput: 0: 910.9. Samples: 304106. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:39:49,442][00731] Avg episode reward: [(0, '5.775')]\u001b[0m\n","\u001b[36m[2024-07-24 09:39:54,437][00731] Fps is (10 sec: 2867.6, 60 sec: 3550.0, 300 sec: 3582.3). Total num frames: 1236992. Throughput: 0: 871.2. Samples: 308830. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:39:54,441][00731] Avg episode reward: [(0, '5.841')]\u001b[0m\n","\u001b[36m[2024-07-24 09:39:59,437][00731] Fps is (10 sec: 4096.1, 60 sec: 3686.4, 300 sec: 3596.1). Total num frames: 1261568. Throughput: 0: 924.1. Samples: 315342. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:39:59,439][00731] Avg episode reward: [(0, '5.409')]\u001b[0m\n","\u001b[36m[2024-07-24 09:40:04,437][00731] Fps is (10 sec: 3686.4, 60 sec: 3549.9, 300 sec: 3596.2). Total num frames: 1273856. Throughput: 0: 933.1. Samples: 318172. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:40:04,442][00731] Avg episode reward: [(0, '5.260')]\u001b[0m\n","\u001b[36m[2024-07-24 09:40:09,437][00731] Fps is (10 sec: 2867.2, 60 sec: 3481.6, 300 sec: 3596.1). Total num frames: 1290240. Throughput: 0: 872.9. Samples: 322164. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:40:09,441][00731] Avg episode reward: [(0, '5.317')]\u001b[0m\n","\u001b[36m[2024-07-24 09:40:14,437][00731] Fps is (10 sec: 3686.4, 60 sec: 3618.1, 300 sec: 3582.3). Total num frames: 1310720. Throughput: 0: 897.4. Samples: 328494. Policy #0 lag: (min: 0.0, avg: 0.6, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:40:14,439][00731] Avg episode reward: [(0, '5.272')]\u001b[0m\n","\u001b[36m[2024-07-24 09:40:19,437][00731] Fps is (10 sec: 4096.1, 60 sec: 3686.4, 300 sec: 3596.1). Total num frames: 1331200. Throughput: 0: 922.4. Samples: 331590. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:40:19,439][00731] Avg episode reward: [(0, '5.194')]\u001b[0m\n","\u001b[36m[2024-07-24 09:40:24,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3481.6, 300 sec: 3596.1). Total num frames: 1343488. Throughput: 0: 890.0. Samples: 336146. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:40:24,440][00731] Avg episode reward: [(0, '5.395')]\u001b[0m\n","\u001b[36m[2024-07-24 09:40:29,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3550.1, 300 sec: 3596.2). Total num frames: 1363968. Throughput: 0: 871.2. Samples: 341524. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:40:29,439][00731] Avg episode reward: [(0, '5.470')]\u001b[0m\n","\u001b[36m[2024-07-24 09:40:34,437][00731] Fps is (10 sec: 4096.0, 60 sec: 3686.4, 300 sec: 3596.2). Total num frames: 1384448. Throughput: 0: 902.8. Samples: 344732. Policy #0 lag: (min: 0.0, avg: 0.5, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:40:34,442][00731] Avg episode reward: [(0, '5.881')]\u001b[0m\n","\u001b[36m[2024-07-24 09:40:39,437][00731] Fps is (10 sec: 3686.4, 60 sec: 3549.9, 300 sec: 3596.2). Total num frames: 1400832. Throughput: 0: 922.9. Samples: 350362. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:40:39,440][00731] Avg episode reward: [(0, '5.875')]\u001b[0m\n","\u001b[36m[2024-07-24 09:40:44,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3481.7, 300 sec: 3596.1). Total num frames: 1417216. Throughput: 0: 874.3. Samples: 354686. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:40:44,439][00731] Avg episode reward: [(0, '5.807')]\u001b[0m\n","\u001b[36m[2024-07-24 09:40:49,437][00731] Fps is (10 sec: 3686.4, 60 sec: 3618.2, 300 sec: 3596.1). Total num frames: 1437696. Throughput: 0: 879.1. Samples: 357732. Policy #0 lag: (min: 0.0, avg: 0.4, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:40:49,440][00731] Avg episode reward: [(0, '5.959')]\u001b[0m\n","\u001b[36m[2024-07-24 09:40:54,437][00731] Fps is (10 sec: 4096.0, 60 sec: 3686.4, 300 sec: 3610.0). Total num frames: 1458176. Throughput: 0: 935.7. Samples: 364270. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:40:54,441][00731] Avg episode reward: [(0, '6.196')]\u001b[0m\n","\u001b[36m[2024-07-24 09:40:59,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3481.6, 300 sec: 3596.1). Total num frames: 1470464. Throughput: 0: 884.6. Samples: 368300. Policy #0 lag: (min: 0.0, avg: 0.4, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:40:59,439][00731] Avg episode reward: [(0, '6.403')]\u001b[0m\n","\u001b[36m[2024-07-24 09:41:04,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3618.1, 300 sec: 3596.1). Total num frames: 1490944. Throughput: 0: 874.0. Samples: 370918. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:41:04,439][00731] Avg episode reward: [(0, '6.430')]\u001b[0m\n","\u001b[36m[2024-07-24 09:41:09,437][00731] Fps is (10 sec: 4096.0, 60 sec: 3686.4, 300 sec: 3596.1). Total num frames: 1511424. Throughput: 0: 916.4. Samples: 377384. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:41:09,442][00731] Avg episode reward: [(0, '6.137')]\u001b[0m\n","\u001b[36m[2024-07-24 09:41:14,440][00731] Fps is (10 sec: 3685.2, 60 sec: 3617.9, 300 sec: 3610.0). Total num frames: 1527808. Throughput: 0: 907.5. Samples: 382364. Policy #0 lag: (min: 0.0, avg: 0.4, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:41:14,447][00731] Avg episode reward: [(0, '6.251')]\u001b[0m\n","\u001b[36m[2024-07-24 09:41:19,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3549.9, 300 sec: 3610.0). Total num frames: 1544192. Throughput: 0: 880.3. Samples: 384346. Policy #0 lag: (min: 0.0, avg: 0.4, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:41:19,439][00731] Avg episode reward: [(0, '6.396')]\u001b[0m\n","\u001b[36m[2024-07-24 09:41:24,437][00731] Fps is (10 sec: 3687.6, 60 sec: 3686.4, 300 sec: 3596.1). Total num frames: 1564672. Throughput: 0: 893.4. Samples: 390566. Policy #0 lag: (min: 0.0, avg: 0.4, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:41:24,439][00731] Avg episode reward: [(0, '7.005')]\u001b[0m\n","\u001b[36m[2024-07-24 09:41:29,439][00731] Fps is (10 sec: 4095.0, 60 sec: 3686.2, 300 sec: 3610.0). Total num frames: 1585152. Throughput: 0: 931.0. Samples: 396582. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:41:29,442][00731] Avg episode reward: [(0, '7.384')]\u001b[0m\n","\u001b[36m[2024-07-24 09:41:34,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3549.9, 300 sec: 3610.0). Total num frames: 1597440. Throughput: 0: 905.7. Samples: 398490. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:41:34,440][00731] Avg episode reward: [(0, '6.971')]\u001b[0m\n","\u001b[36m[2024-07-24 09:41:39,437][00731] Fps is (10 sec: 3277.6, 60 sec: 3618.1, 300 sec: 3596.1). Total num frames: 1617920. Throughput: 0: 878.4. Samples: 403796. Policy #0 lag: (min: 0.0, avg: 0.4, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:41:39,442][00731] Avg episode reward: [(0, '7.192')]\u001b[0m\n","\u001b[36m[2024-07-24 09:41:44,437][00731] Fps is (10 sec: 4096.0, 60 sec: 3686.4, 300 sec: 3596.1). Total num frames: 1638400. Throughput: 0: 933.6. Samples: 410310. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:41:44,439][00731] Avg episode reward: [(0, '7.468')]\u001b[0m\n","\u001b[36m[2024-07-24 09:41:49,437][00731] Fps is (10 sec: 3686.4, 60 sec: 3618.1, 300 sec: 3610.0). Total num frames: 1654784. Throughput: 0: 928.9. Samples: 412718. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:41:49,443][00731] Avg episode reward: [(0, '7.352')]\u001b[0m\n","\u001b[36m[2024-07-24 09:41:54,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3549.9, 300 sec: 3610.0). Total num frames: 1671168. Throughput: 0: 880.0. Samples: 416982. Policy #0 lag: (min: 0.0, avg: 0.5, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:41:54,444][00731] Avg episode reward: [(0, '8.102')]\u001b[0m\n","\u001b[36m[2024-07-24 09:41:59,437][00731] Fps is (10 sec: 3686.4, 60 sec: 3686.4, 300 sec: 3596.1). Total num frames: 1691648. Throughput: 0: 911.6. Samples: 423382. Policy #0 lag: (min: 0.0, avg: 0.7, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:41:59,440][00731] Avg episode reward: [(0, '8.009')]\u001b[0m\n","\u001b[36m[2024-07-24 09:42:04,437][00731] Fps is (10 sec: 4096.0, 60 sec: 3686.4, 300 sec: 3610.1). Total num frames: 1712128. Throughput: 0: 939.2. Samples: 426612. Policy #0 lag: (min: 0.0, avg: 0.5, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:42:04,443][00731] Avg episode reward: [(0, '8.171')]\u001b[0m\n","\u001b[36m[2024-07-24 09:42:09,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3549.9, 300 sec: 3610.0). Total num frames: 1724416. Throughput: 0: 891.9. Samples: 430700. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:42:09,439][00731] Avg episode reward: [(0, '8.076')]\u001b[0m\n","\u001b[36m[2024-07-24 09:42:14,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3618.3, 300 sec: 3610.1). Total num frames: 1744896. Throughput: 0: 886.8. Samples: 436484. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:42:14,439][00731] Avg episode reward: [(0, '8.967')]\u001b[0m\n","\u001b[36m[2024-07-24 09:42:19,437][00731] Fps is (10 sec: 4096.0, 60 sec: 3686.4, 300 sec: 3610.0). Total num frames: 1765376. Throughput: 0: 912.4. Samples: 439548. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:42:19,439][00731] Avg episode reward: [(0, '9.152')]\u001b[0m\n","\u001b[36m[2024-07-24 09:42:24,438][00731] Fps is (10 sec: 3276.3, 60 sec: 3549.8, 300 sec: 3596.1). Total num frames: 1777664. Throughput: 0: 909.2. Samples: 444710. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:42:24,446][00731] Avg episode reward: [(0, '9.048')]\u001b[0m\n","\u001b[36m[2024-07-24 09:42:29,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3550.0, 300 sec: 3610.0). Total num frames: 1798144. Throughput: 0: 876.4. Samples: 449750. Policy #0 lag: (min: 0.0, avg: 0.6, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:42:29,444][00731] Avg episode reward: [(0, '9.163')]\u001b[0m\n","\u001b[36m[2024-07-24 09:42:34,437][00731] Fps is (10 sec: 4096.6, 60 sec: 3686.4, 300 sec: 3610.0). Total num frames: 1818624. Throughput: 0: 894.9. Samples: 452990. Policy #0 lag: (min: 0.0, avg: 0.5, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:42:34,441][00731] Avg episode reward: [(0, '9.454')]\u001b[0m\n","\u001b[36m[2024-07-24 09:42:39,437][00731] Fps is (10 sec: 3686.4, 60 sec: 3618.1, 300 sec: 3610.0). Total num frames: 1835008. Throughput: 0: 933.2. Samples: 458974. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:42:39,442][00731] Avg episode reward: [(0, '9.780')]\u001b[0m\n","\u001b[36m[2024-07-24 09:42:44,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3549.9, 300 sec: 3610.0). Total num frames: 1851392. Throughput: 0: 884.3. Samples: 463174. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:42:44,439][00731] Avg episode reward: [(0, '9.356')]\u001b[0m\n","\u001b[36m[2024-07-24 09:42:49,437][00731] Fps is (10 sec: 3686.4, 60 sec: 3618.1, 300 sec: 3610.0). Total num frames: 1871872. Throughput: 0: 882.8. Samples: 466340. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:42:49,443][00731] Avg episode reward: [(0, '9.068')]\u001b[0m\n","\u001b[36m[2024-07-24 09:42:54,437][00731] Fps is (10 sec: 4095.9, 60 sec: 3686.4, 300 sec: 3610.0). Total num frames: 1892352. Throughput: 0: 937.5. Samples: 472886. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:42:54,439][00731] Avg episode reward: [(0, '9.520')]\u001b[0m\n","\u001b[36m[2024-07-24 09:42:59,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3549.9, 300 sec: 3610.0). Total num frames: 1904640. Throughput: 0: 901.5. Samples: 477052. Policy #0 lag: (min: 0.0, avg: 0.5, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:42:59,444][00731] Avg episode reward: [(0, '9.677')]\u001b[0m\n","\u001b[36m[2024-07-24 09:43:04,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3549.9, 300 sec: 3610.0). Total num frames: 1925120. Throughput: 0: 892.1. Samples: 479694. Policy #0 lag: (min: 0.0, avg: 0.5, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:43:04,444][00731] Avg episode reward: [(0, '10.355')]\u001b[0m\n","\u001b[36m[2024-07-24 09:43:09,437][00731] Fps is (10 sec: 4505.6, 60 sec: 3754.7, 300 sec: 3623.9). Total num frames: 1949696. Throughput: 0: 925.6. Samples: 486362. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:43:09,440][00731] Avg episode reward: [(0, '10.228')]\u001b[0m\n","\u001b[36m[2024-07-24 09:43:14,438][00731] Fps is (10 sec: 3685.9, 60 sec: 3618.0, 300 sec: 3610.0). Total num frames: 1961984. Throughput: 0: 922.5. Samples: 491266. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:43:14,445][00731] Avg episode reward: [(0, '11.098')]\u001b[0m\n","\u001b[36m[2024-07-24 09:43:19,437][00731] Fps is (10 sec: 2867.2, 60 sec: 3549.9, 300 sec: 3623.9). Total num frames: 1978368. Throughput: 0: 894.1. Samples: 493224. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:43:19,439][00731] Avg episode reward: [(0, '11.135')]\u001b[0m\n","\u001b[36m[2024-07-24 09:43:24,437][00731] Fps is (10 sec: 4096.6, 60 sec: 3754.8, 300 sec: 3623.9). Total num frames: 2002944. Throughput: 0: 899.0. Samples: 499428. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:43:24,443][00731] Avg episode reward: [(0, '11.397')]\u001b[0m\n","\u001b[36m[2024-07-24 09:43:29,443][00731] Fps is (10 sec: 4093.5, 60 sec: 3686.0, 300 sec: 3610.0). Total num frames: 2019328. Throughput: 0: 935.4. Samples: 505274. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:43:29,446][00731] Avg episode reward: [(0, '11.923')]\u001b[0m\n","\u001b[36m[2024-07-24 09:43:34,437][00731] Fps is (10 sec: 2867.2, 60 sec: 3549.9, 300 sec: 3610.0). Total num frames: 2031616. Throughput: 0: 909.3. Samples: 507260. Policy #0 lag: (min: 0.0, avg: 0.5, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:43:34,447][00731] Avg episode reward: [(0, '11.932')]\u001b[0m\n","\u001b[36m[2024-07-24 09:43:39,437][00731] Fps is (10 sec: 3278.7, 60 sec: 3618.1, 300 sec: 3610.0). Total num frames: 2052096. Throughput: 0: 883.2. Samples: 512630. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:43:39,441][00731] Avg episode reward: [(0, '13.003')]\u001b[0m\n","\u001b[36m[2024-07-24 09:43:44,437][00731] Fps is (10 sec: 4505.6, 60 sec: 3754.7, 300 sec: 3623.9). Total num frames: 2076672. Throughput: 0: 934.2. Samples: 519090. Policy #0 lag: (min: 0.0, avg: 0.4, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:43:44,441][00731] Avg episode reward: [(0, '12.908')]\u001b[0m\n","\u001b[36m[2024-07-24 09:43:49,441][00731] Fps is (10 sec: 3684.9, 60 sec: 3617.9, 300 sec: 3610.0). Total num frames: 2088960. Throughput: 0: 928.3. Samples: 521470. Policy #0 lag: (min: 0.0, avg: 0.5, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:43:49,444][00731] Avg episode reward: [(0, '12.716')]\u001b[0m\n","\u001b[36m[2024-07-24 09:43:54,437][00731] Fps is (10 sec: 2867.2, 60 sec: 3549.9, 300 sec: 3610.0). Total num frames: 2105344. Throughput: 0: 871.3. Samples: 525572. Policy #0 lag: (min: 0.0, avg: 0.4, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:43:54,439][00731] Avg episode reward: [(0, '13.068')]\u001b[0m\n","\u001b[36m[2024-07-24 09:43:59,437][00731] Fps is (10 sec: 3688.1, 60 sec: 3686.4, 300 sec: 3610.0). Total num frames: 2125824. Throughput: 0: 900.7. Samples: 531798. Policy #0 lag: (min: 0.0, avg: 0.6, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:43:59,439][00731] Avg episode reward: [(0, '14.145')]\u001b[0m\n","\u001b[36m[2024-07-24 09:44:04,439][00731] Fps is (10 sec: 3685.5, 60 sec: 3618.0, 300 sec: 3596.1). Total num frames: 2142208. Throughput: 0: 929.7. Samples: 535062. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:44:04,444][00731] Avg episode reward: [(0, '15.027')]\u001b[0m\n","\u001b[36m[2024-07-24 09:44:09,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3481.6, 300 sec: 3610.0). Total num frames: 2158592. Throughput: 0: 886.1. Samples: 539302. Policy #0 lag: (min: 0.0, avg: 0.6, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:44:09,444][00731] Avg episode reward: [(0, '14.838')]\u001b[0m\n","\u001b[36m[2024-07-24 09:44:14,437][00731] Fps is (10 sec: 3687.3, 60 sec: 3618.2, 300 sec: 3623.9). Total num frames: 2179072. Throughput: 0: 884.3. Samples: 545060. Policy #0 lag: (min: 0.0, avg: 0.5, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:44:14,444][00731] Avg episode reward: [(0, '14.614')]\u001b[0m\n","\u001b[36m[2024-07-24 09:44:19,440][00731] Fps is (10 sec: 4094.5, 60 sec: 3686.2, 300 sec: 3610.0). Total num frames: 2199552. Throughput: 0: 910.0. Samples: 548212. Policy #0 lag: (min: 0.0, avg: 0.4, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:44:19,447][00731] Avg episode reward: [(0, '13.245')]\u001b[0m\n","\u001b[36m[2024-07-24 09:44:24,437][00731] Fps is (10 sec: 3276.7, 60 sec: 3481.6, 300 sec: 3596.2). Total num frames: 2211840. Throughput: 0: 899.5. Samples: 553106. Policy #0 lag: (min: 0.0, avg: 0.6, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:44:24,440][00731] Avg episode reward: [(0, '13.371')]\u001b[0m\n","\u001b[36m[2024-07-24 09:44:29,437][00731] Fps is (10 sec: 3278.0, 60 sec: 3550.2, 300 sec: 3623.9). Total num frames: 2232320. Throughput: 0: 864.4. Samples: 557988. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:44:29,439][00731] Avg episode reward: [(0, '14.107')]\u001b[0m\n","\u001b[36m[2024-07-24 09:44:34,437][00731] Fps is (10 sec: 4096.1, 60 sec: 3686.4, 300 sec: 3610.0). Total num frames: 2252800. Throughput: 0: 886.4. Samples: 561352. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:44:34,445][00731] Avg episode reward: [(0, '14.101')]\u001b[0m\n","\u001b[36m[2024-07-24 09:44:39,437][00731] Fps is (10 sec: 3686.4, 60 sec: 3618.2, 300 sec: 3596.2). Total num frames: 2269184. Throughput: 0: 927.8. Samples: 567322. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:44:39,442][00731] Avg episode reward: [(0, '14.544')]\u001b[0m\n","\u001b[36m[2024-07-24 09:44:44,437][00731] Fps is (10 sec: 3276.7, 60 sec: 3481.6, 300 sec: 3610.0). Total num frames: 2285568. Throughput: 0: 884.4. Samples: 571598. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:44:44,444][00731] Avg episode reward: [(0, '13.511')]\u001b[0m\n","\u001b[36m[2024-07-24 09:44:49,437][00731] Fps is (10 sec: 3686.4, 60 sec: 3618.4, 300 sec: 3623.9). Total num frames: 2306048. Throughput: 0: 886.5. Samples: 574950. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:44:49,444][00731] Avg episode reward: [(0, '13.087')]\u001b[0m\n","\u001b[36m[2024-07-24 09:44:54,438][00731] Fps is (10 sec: 4095.4, 60 sec: 3686.3, 300 sec: 3610.0). Total num frames: 2326528. Throughput: 0: 936.2. Samples: 581434. Policy #0 lag: (min: 0.0, avg: 0.5, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:44:54,447][00731] Avg episode reward: [(0, '14.083')]\u001b[0m\n","\u001b[36m[2024-07-24 09:44:59,438][00731] Fps is (10 sec: 3276.2, 60 sec: 3549.8, 300 sec: 3610.0). Total num frames: 2338816. Throughput: 0: 903.3. Samples: 585712. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:44:59,444][00731] Avg episode reward: [(0, '13.711')]\u001b[0m\n","\u001b[36m[2024-07-24 09:45:04,437][00731] Fps is (10 sec: 3277.3, 60 sec: 3618.3, 300 sec: 3623.9). Total num frames: 2359296. Throughput: 0: 892.5. Samples: 588370. Policy #0 lag: (min: 0.0, avg: 0.6, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:45:04,445][00731] Avg episode reward: [(0, '14.581')]\u001b[0m\n","\u001b[36m[2024-07-24 09:45:09,437][00731] Fps is (10 sec: 4506.3, 60 sec: 3754.7, 300 sec: 3637.8). Total num frames: 2383872. Throughput: 0: 930.1. Samples: 594962. Policy #0 lag: (min: 0.0, avg: 0.5, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:45:09,441][00731] Avg episode reward: [(0, '16.466')]\u001b[0m\n","\u001b[36m[2024-07-24 09:45:14,437][00731] Fps is (10 sec: 3686.4, 60 sec: 3618.1, 300 sec: 3610.0). Total num frames: 2396160. Throughput: 0: 933.6. Samples: 599998. Policy #0 lag: (min: 0.0, avg: 0.4, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:45:14,442][00731] Avg episode reward: [(0, '17.697')]\u001b[0m\n","\u001b[36m[2024-07-24 09:45:19,437][00731] Fps is (10 sec: 2867.2, 60 sec: 3550.1, 300 sec: 3623.9). Total num frames: 2412544. Throughput: 0: 902.3. Samples: 601954. Policy #0 lag: (min: 0.0, avg: 0.4, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:45:19,440][00731] Avg episode reward: [(0, '18.040')]\u001b[0m\n","\u001b[36m[2024-07-24 09:45:24,437][00731] Fps is (10 sec: 4096.0, 60 sec: 3754.7, 300 sec: 3637.8). Total num frames: 2437120. Throughput: 0: 907.2. Samples: 608148. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:45:24,445][00731] Avg episode reward: [(0, '19.018')]\u001b[0m\n","\u001b[36m[2024-07-24 09:45:29,438][00731] Fps is (10 sec: 4095.4, 60 sec: 3686.3, 300 sec: 3623.9). Total num frames: 2453504. Throughput: 0: 939.8. Samples: 613890. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:45:29,444][00731] Avg episode reward: [(0, '17.860')]\u001b[0m\n","\u001b[36m[2024-07-24 09:45:34,437][00731] Fps is (10 sec: 2867.1, 60 sec: 3549.8, 300 sec: 3610.0). Total num frames: 2465792. Throughput: 0: 911.7. Samples: 615978. Policy #0 lag: (min: 0.0, avg: 0.4, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:45:34,440][00731] Avg episode reward: [(0, '17.154')]\u001b[0m\n","\u001b[36m[2024-07-24 09:45:39,437][00731] Fps is (10 sec: 3686.9, 60 sec: 3686.4, 300 sec: 3637.8). Total num frames: 2490368. Throughput: 0: 891.6. Samples: 621554. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:45:39,441][00731] Avg episode reward: [(0, '15.625')]\u001b[0m\n","\u001b[36m[2024-07-24 09:45:44,437][00731] Fps is (10 sec: 4505.7, 60 sec: 3754.7, 300 sec: 3637.8). Total num frames: 2510848. Throughput: 0: 944.2. Samples: 628200. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:45:44,442][00731] Avg episode reward: [(0, '16.037')]\u001b[0m\n","\u001b[36m[2024-07-24 09:45:49,440][00731] Fps is (10 sec: 3275.8, 60 sec: 3617.9, 300 sec: 3610.0). Total num frames: 2523136. Throughput: 0: 933.6. Samples: 630386. Policy #0 lag: (min: 0.0, avg: 0.6, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:45:49,442][00731] Avg episode reward: [(0, '15.533')]\u001b[0m\n","\u001b[36m[2024-07-24 09:45:54,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3618.2, 300 sec: 3637.8). Total num frames: 2543616. Throughput: 0: 888.4. Samples: 634940. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:45:54,445][00731] Avg episode reward: [(0, '17.623')]\u001b[0m\n","\u001b[36m[2024-07-24 09:45:59,437][00731] Fps is (10 sec: 4097.4, 60 sec: 3754.8, 300 sec: 3637.8). Total num frames: 2564096. Throughput: 0: 925.0. Samples: 641622. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:45:59,439][00731] Avg episode reward: [(0, '17.077')]\u001b[0m\n","\u001b[36m[2024-07-24 09:46:04,437][00731] Fps is (10 sec: 3686.3, 60 sec: 3686.4, 300 sec: 3623.9). Total num frames: 2580480. Throughput: 0: 953.6. Samples: 644866. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:46:04,443][00731] Avg episode reward: [(0, '17.371')]\u001b[0m\n","\u001b[36m[2024-07-24 09:46:09,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3549.9, 300 sec: 3624.0). Total num frames: 2596864. Throughput: 0: 905.5. Samples: 648896. Policy #0 lag: (min: 0.0, avg: 0.7, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:46:09,443][00731] Avg episode reward: [(0, '18.272')]\u001b[0m\n","\u001b[36m[2024-07-24 09:46:14,437][00731] Fps is (10 sec: 3686.5, 60 sec: 3686.4, 300 sec: 3637.8). Total num frames: 2617344. Throughput: 0: 916.2. Samples: 655116. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:46:14,439][00731] Avg episode reward: [(0, '19.266')]\u001b[0m\n","\u001b[36m[2024-07-24 09:46:19,437][00731] Fps is (10 sec: 4096.0, 60 sec: 3754.7, 300 sec: 3637.8). Total num frames: 2637824. Throughput: 0: 942.1. Samples: 658374. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:46:19,439][00731] Avg episode reward: [(0, '19.026')]\u001b[0m\n","\u001b[36m[2024-07-24 09:46:24,437][00731] Fps is (10 sec: 3686.4, 60 sec: 3618.1, 300 sec: 3623.9). Total num frames: 2654208. Throughput: 0: 923.7. Samples: 663118. Policy #0 lag: (min: 0.0, avg: 0.5, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:46:24,442][00731] Avg episode reward: [(0, '19.451')]\u001b[0m\n","\u001b[36m[2024-07-24 09:46:29,437][00731] Fps is (10 sec: 3686.4, 60 sec: 3686.5, 300 sec: 3651.7). Total num frames: 2674688. Throughput: 0: 895.4. Samples: 668492. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:46:29,439][00731] Avg episode reward: [(0, '19.187')]\u001b[0m\n","\u001b[36m[2024-07-24 09:46:34,437][00731] Fps is (10 sec: 4096.0, 60 sec: 3823.0, 300 sec: 3651.7). Total num frames: 2695168. Throughput: 0: 919.9. Samples: 671780. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:46:34,444][00731] Avg episode reward: [(0, '17.718')]\u001b[0m\n","\u001b[36m[2024-07-24 09:46:39,437][00731] Fps is (10 sec: 3686.4, 60 sec: 3686.4, 300 sec: 3637.8). Total num frames: 2711552. Throughput: 0: 946.6. Samples: 677536. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:46:39,439][00731] Avg episode reward: [(0, '17.781')]\u001b[0m\n","\u001b[36m[2024-07-24 09:46:44,437][00731] Fps is (10 sec: 3276.7, 60 sec: 3618.1, 300 sec: 3637.8). Total num frames: 2727936. Throughput: 0: 898.8. Samples: 682070. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:46:44,442][00731] Avg episode reward: [(0, '18.467')]\u001b[0m\n","\u001b[36m[2024-07-24 09:46:49,437][00731] Fps is (10 sec: 3686.4, 60 sec: 3754.9, 300 sec: 3651.7). Total num frames: 2748416. Throughput: 0: 901.7. Samples: 685440. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:46:49,439][00731] Avg episode reward: [(0, '18.297')]\u001b[0m\n","\u001b[36m[2024-07-24 09:46:54,437][00731] Fps is (10 sec: 4096.1, 60 sec: 3754.7, 300 sec: 3651.7). Total num frames: 2768896. Throughput: 0: 951.8. Samples: 691726. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:46:54,439][00731] Avg episode reward: [(0, '17.737')]\u001b[0m\n","\u001b[36m[2024-07-24 09:46:59,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3618.1, 300 sec: 3623.9). Total num frames: 2781184. Throughput: 0: 904.9. Samples: 695836. Policy #0 lag: (min: 0.0, avg: 0.4, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:46:59,444][00731] Avg episode reward: [(0, '18.493')]\u001b[0m\n","\u001b[36m[2024-07-24 09:47:04,437][00731] Fps is (10 sec: 3686.4, 60 sec: 3754.7, 300 sec: 3665.6). Total num frames: 2805760. Throughput: 0: 897.4. Samples: 698758. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:47:04,441][00731] Avg episode reward: [(0, '18.708')]\u001b[0m\n","\u001b[36m[2024-07-24 09:47:09,437][00731] Fps is (10 sec: 4505.6, 60 sec: 3822.9, 300 sec: 3665.6). Total num frames: 2826240. Throughput: 0: 938.8. Samples: 705362. Policy #0 lag: (min: 0.0, avg: 0.4, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:47:09,441][00731] Avg episode reward: [(0, '17.241')]\u001b[0m\n","\u001b[36m[2024-07-24 09:47:14,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3686.4, 300 sec: 3637.8). Total num frames: 2838528. Throughput: 0: 929.6. Samples: 710324. Policy #0 lag: (min: 0.0, avg: 0.5, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:47:14,445][00731] Avg episode reward: [(0, '18.881')]\u001b[0m\n","\u001b[36m[2024-07-24 09:47:19,437][00731] Fps is (10 sec: 2867.1, 60 sec: 3618.1, 300 sec: 3651.7). Total num frames: 2854912. Throughput: 0: 902.3. Samples: 712384. Policy #0 lag: (min: 0.0, avg: 0.5, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:47:19,440][00731] Avg episode reward: [(0, '19.079')]\u001b[0m\n","\u001b[36m[2024-07-24 09:47:24,437][00731] Fps is (10 sec: 4095.8, 60 sec: 3754.6, 300 sec: 3665.6). Total num frames: 2879488. Throughput: 0: 911.5. Samples: 718554. Policy #0 lag: (min: 0.0, avg: 0.6, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:47:24,439][00731] Avg episode reward: [(0, '19.902')]\u001b[0m\n","\u001b[36m[2024-07-24 09:47:29,437][00731] Fps is (10 sec: 4096.2, 60 sec: 3686.4, 300 sec: 3651.7). Total num frames: 2895872. Throughput: 0: 942.3. Samples: 724474. Policy #0 lag: (min: 0.0, avg: 0.4, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:47:29,442][00731] Avg episode reward: [(0, '19.384')]\u001b[0m\n","\u001b[36m[2024-07-24 09:47:34,440][00731] Fps is (10 sec: 3275.9, 60 sec: 3617.9, 300 sec: 3651.6). Total num frames: 2912256. Throughput: 0: 912.7. Samples: 726514. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:47:34,444][00731] Avg episode reward: [(0, '19.048')]\u001b[0m\n","\u001b[36m[2024-07-24 09:47:39,437][00731] Fps is (10 sec: 3686.4, 60 sec: 3686.4, 300 sec: 3665.6). Total num frames: 2932736. Throughput: 0: 898.1. Samples: 732140. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:47:39,440][00731] Avg episode reward: [(0, '17.541')]\u001b[0m\n","\u001b[36m[2024-07-24 09:47:44,437][00731] Fps is (10 sec: 4097.3, 60 sec: 3754.7, 300 sec: 3665.6). Total num frames: 2953216. Throughput: 0: 955.3. Samples: 738824. Policy #0 lag: (min: 0.0, avg: 0.4, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:47:44,439][00731] Avg episode reward: [(0, '19.038')]\u001b[0m\n","\u001b[36m[2024-07-24 09:47:49,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3618.1, 300 sec: 3637.8). Total num frames: 2965504. Throughput: 0: 938.5. Samples: 740990. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:47:49,442][00731] Avg episode reward: [(0, '19.259')]\u001b[0m\n","\u001b[36m[2024-07-24 09:47:54,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3618.1, 300 sec: 3665.6). Total num frames: 2985984. Throughput: 0: 894.5. Samples: 745614. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:47:54,444][00731] Avg episode reward: [(0, '19.344')]\u001b[0m\n","\u001b[36m[2024-07-24 09:47:59,437][00731] Fps is (10 sec: 4096.0, 60 sec: 3754.7, 300 sec: 3665.6). Total num frames: 3006464. Throughput: 0: 931.4. Samples: 752238. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:47:59,444][00731] Avg episode reward: [(0, '20.017')]\u001b[0m\n","\u001b[36m[2024-07-24 09:48:04,437][00731] Fps is (10 sec: 3686.4, 60 sec: 3618.1, 300 sec: 3637.8). Total num frames: 3022848. Throughput: 0: 954.7. Samples: 755344. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:48:04,439][00731] Avg episode reward: [(0, '20.857')]\u001b[0m\n","\u001b[36m[2024-07-24 09:48:09,438][00731] Fps is (10 sec: 3276.3, 60 sec: 3549.8, 300 sec: 3651.7). Total num frames: 3039232. Throughput: 0: 905.7. Samples: 759310. Policy #0 lag: (min: 0.0, avg: 0.4, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:48:09,441][00731] Avg episode reward: [(0, '20.977')]\u001b[0m\n","\u001b[36m[2024-07-24 09:48:14,437][00731] Fps is (10 sec: 3686.4, 60 sec: 3686.4, 300 sec: 3665.6). Total num frames: 3059712. Throughput: 0: 915.8. Samples: 765684. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:48:14,439][00731] Avg episode reward: [(0, '19.898')]\u001b[0m\n","\u001b[36m[2024-07-24 09:48:19,437][00731] Fps is (10 sec: 4096.7, 60 sec: 3754.7, 300 sec: 3651.7). Total num frames: 3080192. Throughput: 0: 944.9. Samples: 769030. Policy #0 lag: (min: 0.0, avg: 0.6, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:48:19,440][00731] Avg episode reward: [(0, '20.079')]\u001b[0m\n","\u001b[36m[2024-07-24 09:48:24,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3549.9, 300 sec: 3637.9). Total num frames: 3092480. Throughput: 0: 921.3. Samples: 773598. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:48:24,439][00731] Avg episode reward: [(0, '18.783')]\u001b[0m\n","\u001b[36m[2024-07-24 09:48:29,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3618.1, 300 sec: 3665.6). Total num frames: 3112960. Throughput: 0: 891.5. Samples: 778942. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:48:29,440][00731] Avg episode reward: [(0, '17.967')]\u001b[0m\n","\u001b[36m[2024-07-24 09:48:34,437][00731] Fps is (10 sec: 4096.0, 60 sec: 3686.6, 300 sec: 3665.6). Total num frames: 3133440. Throughput: 0: 914.7. Samples: 782150. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:48:34,443][00731] Avg episode reward: [(0, '17.659')]\u001b[0m\n","\u001b[36m[2024-07-24 09:48:39,437][00731] Fps is (10 sec: 3686.4, 60 sec: 3618.1, 300 sec: 3637.8). Total num frames: 3149824. Throughput: 0: 925.3. Samples: 787254. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:48:39,440][00731] Avg episode reward: [(0, '17.806')]\u001b[0m\n","\u001b[36m[2024-07-24 09:48:44,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3549.9, 300 sec: 3651.7). Total num frames: 3166208. Throughput: 0: 876.0. Samples: 791658. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:48:44,441][00731] Avg episode reward: [(0, '17.881')]\u001b[0m\n","\u001b[36m[2024-07-24 09:48:49,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3618.1, 300 sec: 3651.7). Total num frames: 3182592. Throughput: 0: 872.0. Samples: 794582. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:48:49,447][00731] Avg episode reward: [(0, '19.938')]\u001b[0m\n","\u001b[36m[2024-07-24 09:48:54,437][00731] Fps is (10 sec: 3686.4, 60 sec: 3618.1, 300 sec: 3651.7). Total num frames: 3203072. Throughput: 0: 911.9. Samples: 800342. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:48:54,442][00731] Avg episode reward: [(0, '20.080')]\u001b[0m\n","\u001b[36m[2024-07-24 09:48:59,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3481.6, 300 sec: 3637.8). Total num frames: 3215360. Throughput: 0: 857.1. Samples: 804252. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:48:59,440][00731] Avg episode reward: [(0, '20.470')]\u001b[0m\n","\u001b[36m[2024-07-24 09:49:04,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3549.9, 300 sec: 3651.7). Total num frames: 3235840. Throughput: 0: 842.5. Samples: 806942. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:49:04,439][00731] Avg episode reward: [(0, '19.904')]\u001b[0m\n","\u001b[36m[2024-07-24 09:49:09,437][00731] Fps is (10 sec: 4096.0, 60 sec: 3618.2, 300 sec: 3651.7). Total num frames: 3256320. Throughput: 0: 875.5. Samples: 812996. Policy #0 lag: (min: 0.0, avg: 0.5, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:49:09,445][00731] Avg episode reward: [(0, '19.678')]\u001b[0m\n","\u001b[36m[2024-07-24 09:49:14,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3481.6, 300 sec: 3624.0). Total num frames: 3268608. Throughput: 0: 857.4. Samples: 817524. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:49:14,439][00731] Avg episode reward: [(0, '19.385')]\u001b[0m\n","\u001b[36m[2024-07-24 09:49:19,437][00731] Fps is (10 sec: 2867.2, 60 sec: 3413.3, 300 sec: 3637.8). Total num frames: 3284992. Throughput: 0: 827.5. Samples: 819388. Policy #0 lag: (min: 0.0, avg: 0.4, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:49:19,443][00731] Avg episode reward: [(0, '19.428')]\u001b[0m\n","\u001b[36m[2024-07-24 09:49:24,437][00731] Fps is (10 sec: 3686.4, 60 sec: 3549.9, 300 sec: 3637.8). Total num frames: 3305472. Throughput: 0: 845.1. Samples: 825282. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:49:24,444][00731] Avg episode reward: [(0, '19.232')]\u001b[0m\n","\u001b[36m[2024-07-24 09:49:29,443][00731] Fps is (10 sec: 3684.2, 60 sec: 3481.3, 300 sec: 3623.8). Total num frames: 3321856. Throughput: 0: 868.4. Samples: 830740. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:49:29,450][00731] Avg episode reward: [(0, '20.115')]\u001b[0m\n","\u001b[36m[2024-07-24 09:49:34,437][00731] Fps is (10 sec: 2867.2, 60 sec: 3345.1, 300 sec: 3610.0). Total num frames: 3334144. Throughput: 0: 845.0. Samples: 832608. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:49:34,444][00731] Avg episode reward: [(0, '21.514')]\u001b[0m\n","\u001b[36m[2024-07-24 09:49:39,437][00731] Fps is (10 sec: 3278.6, 60 sec: 3413.3, 300 sec: 3623.9). Total num frames: 3354624. Throughput: 0: 836.0. Samples: 837960. Policy #0 lag: (min: 0.0, avg: 0.6, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:49:39,444][00731] Avg episode reward: [(0, '23.787')]\u001b[0m\n","\u001b[36m[2024-07-24 09:49:44,437][00731] Fps is (10 sec: 4096.0, 60 sec: 3481.6, 300 sec: 3623.9). Total num frames: 3375104. Throughput: 0: 891.5. Samples: 844368. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:49:44,444][00731] Avg episode reward: [(0, '23.892')]\u001b[0m\n","\u001b[36m[2024-07-24 09:49:49,437][00731] Fps is (10 sec: 3276.9, 60 sec: 3413.3, 300 sec: 3596.2). Total num frames: 3387392. Throughput: 0: 874.0. Samples: 846272. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:49:49,439][00731] Avg episode reward: [(0, '23.593')]\u001b[0m\n","\u001b[36m[2024-07-24 09:49:54,437][00731] Fps is (10 sec: 2867.2, 60 sec: 3345.1, 300 sec: 3610.1). Total num frames: 3403776. Throughput: 0: 837.6. Samples: 850686. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:49:54,443][00731] Avg episode reward: [(0, '24.781')]\u001b[0m\n","\u001b[36m[2024-07-24 09:49:59,437][00731] Fps is (10 sec: 3686.4, 60 sec: 3481.6, 300 sec: 3610.0). Total num frames: 3424256. Throughput: 0: 877.7. Samples: 857022. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:49:59,442][00731] Avg episode reward: [(0, '23.426')]\u001b[0m\n","\u001b[36m[2024-07-24 09:50:04,437][00731] Fps is (10 sec: 3686.4, 60 sec: 3413.3, 300 sec: 3582.3). Total num frames: 3440640. Throughput: 0: 900.6. Samples: 859914. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:50:04,446][00731] Avg episode reward: [(0, '22.478')]\u001b[0m\n","\u001b[36m[2024-07-24 09:50:09,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3345.1, 300 sec: 3596.1). Total num frames: 3457024. Throughput: 0: 859.0. Samples: 863936. Policy #0 lag: (min: 0.0, avg: 0.5, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:50:09,439][00731] Avg episode reward: [(0, '21.496')]\u001b[0m\n","\u001b[36m[2024-07-24 09:50:14,437][00731] Fps is (10 sec: 4096.0, 60 sec: 3549.9, 300 sec: 3623.9). Total num frames: 3481600. Throughput: 0: 876.5. Samples: 870178. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:50:14,440][00731] Avg episode reward: [(0, '19.914')]\u001b[0m\n","\u001b[36m[2024-07-24 09:50:19,440][00731] Fps is (10 sec: 4094.5, 60 sec: 3549.7, 300 sec: 3596.1). Total num frames: 3497984. Throughput: 0: 897.8. Samples: 873014. Policy #0 lag: (min: 0.0, avg: 0.4, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:50:19,442][00731] Avg episode reward: [(0, '20.185')]\u001b[0m\n","\u001b[36m[2024-07-24 09:50:24,437][00731] Fps is (10 sec: 2457.6, 60 sec: 3345.1, 300 sec: 3568.4). Total num frames: 3506176. Throughput: 0: 873.0. Samples: 877244. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:50:24,439][00731] Avg episode reward: [(0, '20.115')]\u001b[0m\n","\u001b[36m[2024-07-24 09:50:29,437][00731] Fps is (10 sec: 2868.2, 60 sec: 3413.7, 300 sec: 3596.2). Total num frames: 3526656. Throughput: 0: 837.4. Samples: 882050. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:50:29,439][00731] Avg episode reward: [(0, '20.138')]\u001b[0m\n","\u001b[36m[2024-07-24 09:50:34,437][00731] Fps is (10 sec: 4096.0, 60 sec: 3549.9, 300 sec: 3582.3). Total num frames: 3547136. Throughput: 0: 860.9. Samples: 885012. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:50:34,439][00731] Avg episode reward: [(0, '21.042')]\u001b[0m\n","\u001b[36m[2024-07-24 09:50:39,437][00731] Fps is (10 sec: 3686.2, 60 sec: 3481.6, 300 sec: 3568.4). Total num frames: 3563520. Throughput: 0: 884.0. Samples: 890466. Policy #0 lag: (min: 0.0, avg: 0.4, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:50:39,439][00731] Avg episode reward: [(0, '21.415')]\u001b[0m\n","\u001b[36m[2024-07-24 09:50:44,437][00731] Fps is (10 sec: 2867.2, 60 sec: 3345.1, 300 sec: 3568.4). Total num frames: 3575808. Throughput: 0: 831.0. Samples: 894418. Policy #0 lag: (min: 0.0, avg: 0.4, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:50:44,440][00731] Avg episode reward: [(0, '22.001')]\u001b[0m\n","\u001b[36m[2024-07-24 09:50:49,437][00731] Fps is (10 sec: 3276.9, 60 sec: 3481.6, 300 sec: 3568.4). Total num frames: 3596288. Throughput: 0: 833.9. Samples: 897438. Policy #0 lag: (min: 0.0, avg: 0.4, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:50:49,444][00731] Avg episode reward: [(0, '20.999')]\u001b[0m\n","\u001b[36m[2024-07-24 09:50:54,437][00731] Fps is (10 sec: 3686.4, 60 sec: 3481.6, 300 sec: 3554.5). Total num frames: 3612672. Throughput: 0: 875.8. Samples: 903348. Policy #0 lag: (min: 0.0, avg: 0.4, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:50:54,439][00731] Avg episode reward: [(0, '21.218')]\u001b[0m\n","\u001b[36m[2024-07-24 09:50:59,441][00731] Fps is (10 sec: 3275.7, 60 sec: 3413.1, 300 sec: 3554.5). Total num frames: 3629056. Throughput: 0: 825.8. Samples: 907342. Policy #0 lag: (min: 0.0, avg: 0.4, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:50:59,443][00731] Avg episode reward: [(0, '21.329')]\u001b[0m\n","\u001b[36m[2024-07-24 09:51:04,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3413.3, 300 sec: 3554.5). Total num frames: 3645440. Throughput: 0: 813.9. Samples: 909638. Policy #0 lag: (min: 0.0, avg: 0.4, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:51:04,442][00731] Avg episode reward: [(0, '21.266')]\u001b[0m\n","\u001b[36m[2024-07-24 09:51:09,437][00731] Fps is (10 sec: 3687.8, 60 sec: 3481.6, 300 sec: 3554.5). Total num frames: 3665920. Throughput: 0: 860.0. Samples: 915946. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:51:09,440][00731] Avg episode reward: [(0, '20.532')]\u001b[0m\n","\u001b[36m[2024-07-24 09:51:14,439][00731] Fps is (10 sec: 3685.5, 60 sec: 3344.9, 300 sec: 3540.6). Total num frames: 3682304. Throughput: 0: 865.7. Samples: 921008. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:51:14,442][00731] Avg episode reward: [(0, '19.807')]\u001b[0m\n","\u001b[36m[2024-07-24 09:51:19,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3345.3, 300 sec: 3540.6). Total num frames: 3698688. Throughput: 0: 841.6. Samples: 922886. Policy #0 lag: (min: 0.0, avg: 0.4, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:51:19,441][00731] Avg episode reward: [(0, '19.844')]\u001b[0m\n","\u001b[36m[2024-07-24 09:51:24,437][00731] Fps is (10 sec: 3687.3, 60 sec: 3549.9, 300 sec: 3540.6). Total num frames: 3719168. Throughput: 0: 848.0. Samples: 928624. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:51:24,440][00731] Avg episode reward: [(0, '19.901')]\u001b[0m\n","\u001b[36m[2024-07-24 09:51:29,437][00731] Fps is (10 sec: 3686.4, 60 sec: 3481.6, 300 sec: 3526.7). Total num frames: 3735552. Throughput: 0: 891.7. Samples: 934544. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:51:29,442][00731] Avg episode reward: [(0, '19.442')]\u001b[0m\n","\u001b[36m[2024-07-24 09:51:34,437][00731] Fps is (10 sec: 2867.2, 60 sec: 3345.1, 300 sec: 3512.8). Total num frames: 3747840. Throughput: 0: 865.3. Samples: 936378. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:51:34,443][00731] Avg episode reward: [(0, '20.389')]\u001b[0m\n","\u001b[36m[2024-07-24 09:51:39,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3413.4, 300 sec: 3526.7). Total num frames: 3768320. Throughput: 0: 850.4. Samples: 941614. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:51:39,439][00731] Avg episode reward: [(0, '19.851')]\u001b[0m\n","\u001b[36m[2024-07-24 09:51:44,437][00731] Fps is (10 sec: 4505.6, 60 sec: 3618.1, 300 sec: 3540.6). Total num frames: 3792896. Throughput: 0: 903.3. Samples: 947986. Policy #0 lag: (min: 0.0, avg: 0.4, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:51:44,441][00731] Avg episode reward: [(0, '21.239')]\u001b[0m\n","\u001b[36m[2024-07-24 09:51:49,439][00731] Fps is (10 sec: 3685.5, 60 sec: 3481.5, 300 sec: 3512.8). Total num frames: 3805184. Throughput: 0: 901.8. Samples: 950220. Policy #0 lag: (min: 0.0, avg: 0.4, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:51:49,443][00731] Avg episode reward: [(0, '21.520')]\u001b[0m\n","\u001b[36m[2024-07-24 09:51:54,437][00731] Fps is (10 sec: 2867.2, 60 sec: 3481.6, 300 sec: 3526.7). Total num frames: 3821568. Throughput: 0: 857.1. Samples: 954516. Policy #0 lag: (min: 0.0, avg: 0.4, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:51:54,441][00731] Avg episode reward: [(0, '21.556')]\u001b[0m\n","\u001b[36m[2024-07-24 09:51:59,437][00731] Fps is (10 sec: 3687.3, 60 sec: 3550.1, 300 sec: 3512.8). Total num frames: 3842048. Throughput: 0: 882.2. Samples: 960706. Policy #0 lag: (min: 0.0, avg: 0.4, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:51:59,441][00731] Avg episode reward: [(0, '20.827')]\u001b[0m\n","\u001b[36m[2024-07-24 09:52:04,437][00731] Fps is (10 sec: 3686.4, 60 sec: 3549.9, 300 sec: 3499.0). Total num frames: 3858432. Throughput: 0: 908.8. Samples: 963784. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:52:04,443][00731] Avg episode reward: [(0, '21.152')]\u001b[0m\n","\u001b[36m[2024-07-24 09:52:09,437][00731] Fps is (10 sec: 3276.8, 60 sec: 3481.6, 300 sec: 3512.8). Total num frames: 3874816. Throughput: 0: 871.9. Samples: 967858. Policy #0 lag: (min: 0.0, avg: 0.5, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:52:09,443][00731] Avg episode reward: [(0, '22.080')]\u001b[0m\n","\u001b[36m[2024-07-24 09:52:14,437][00731] Fps is (10 sec: 3686.4, 60 sec: 3550.0, 300 sec: 3526.7). Total num frames: 3895296. Throughput: 0: 874.2. Samples: 973884. Policy #0 lag: (min: 0.0, avg: 0.4, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:52:14,445][00731] Avg episode reward: [(0, '21.579')]\u001b[0m\n","\u001b[36m[2024-07-24 09:52:19,437][00731] Fps is (10 sec: 4095.9, 60 sec: 3618.1, 300 sec: 3512.8). Total num frames: 3915776. Throughput: 0: 904.4. Samples: 977076. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:52:19,441][00731] Avg episode reward: [(0, '21.365')]\u001b[0m\n","\u001b[36m[2024-07-24 09:52:24,439][00731] Fps is (10 sec: 3276.0, 60 sec: 3481.5, 300 sec: 3498.9). Total num frames: 3928064. Throughput: 0: 897.1. Samples: 981988. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:52:24,445][00731] Avg episode reward: [(0, '20.330')]\u001b[0m\n","\u001b[36m[2024-07-24 09:52:29,437][00731] Fps is (10 sec: 3276.9, 60 sec: 3549.9, 300 sec: 3512.9). Total num frames: 3948544. Throughput: 0: 870.9. Samples: 987176. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:52:29,439][00731] Avg episode reward: [(0, '20.009')]\u001b[0m\n","\u001b[36m[2024-07-24 09:52:34,437][00731] Fps is (10 sec: 4097.0, 60 sec: 3686.4, 300 sec: 3512.8). Total num frames: 3969024. Throughput: 0: 891.1. Samples: 990316. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:52:34,439][00731] Avg episode reward: [(0, '19.234')]\u001b[0m\n","\u001b[36m[2024-07-24 09:52:39,438][00731] Fps is (10 sec: 3685.8, 60 sec: 3618.0, 300 sec: 3498.9). Total num frames: 3985408. Throughput: 0: 925.7. Samples: 996176. Policy #0 lag: (min: 0.0, avg: 0.6, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:52:39,445][00731] Avg episode reward: [(0, '19.282')]\u001b[0m\n","\u001b[36m[2024-07-24 09:52:44,444][00731] Fps is (10 sec: 3274.4, 60 sec: 3481.2, 300 sec: 3512.8). Total num frames: 4001792. Throughput: 0: 884.8. Samples: 1000530. Policy #0 lag: (min: 0.0, avg: 0.6, max: 1.0)\u001b[0m\n","\u001b[36m[2024-07-24 09:52:44,451][00731] Avg episode reward: [(0, '19.009')]\u001b[0m\n","\u001b[36m[2024-07-24 09:52:45,209][00731] Component Batcher_0 stopped!\u001b[0m\n","\u001b[36m[2024-07-24 09:52:45,280][00731] Component RolloutWorker_w1 stopped!\u001b[0m\n","\u001b[36m[2024-07-24 09:52:45,290][00731] Component RolloutWorker_w3 stopped!\u001b[0m\n","\u001b[36m[2024-07-24 09:52:45,309][00731] Component RolloutWorker_w4 stopped!\u001b[0m\n","\u001b[36m[2024-07-24 09:52:45,330][00731] Component RolloutWorker_w5 stopped!\u001b[0m\n","\u001b[36m[2024-07-24 09:52:45,342][00731] Component RolloutWorker_w7 stopped!\u001b[0m\n","\u001b[36m[2024-07-24 09:52:45,345][00731] Component RolloutWorker_w2 stopped!\u001b[0m\n","\u001b[36m[2024-07-24 09:52:45,348][00731] Component RolloutWorker_w6 stopped!\u001b[0m\n","\u001b[36m[2024-07-24 09:52:45,360][00731] Component RolloutWorker_w0 stopped!\u001b[0m\n","\u001b[36m[2024-07-24 09:52:45,379][00731] Component InferenceWorker_p0-w0 stopped!\u001b[0m\n","\u001b[36m[2024-07-24 09:52:45,608][00731] Component LearnerWorker_p0 stopped!\u001b[0m\n","\u001b[36m[2024-07-24 09:52:45,612][00731] Waiting for process learner_proc0 to stop...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:47,169][00731] Waiting for process inference_proc0-0 to join...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:47,176][00731] Waiting for process rollout_proc0 to join...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:49,038][00731] Waiting for process rollout_proc1 to join...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:49,041][00731] Waiting for process rollout_proc2 to join...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:49,046][00731] Waiting for process rollout_proc3 to join...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:49,049][00731] Waiting for process rollout_proc4 to join...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:49,053][00731] Waiting for process rollout_proc5 to join...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:49,057][00731] Waiting for process rollout_proc6 to join...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:49,061][00731] Waiting for process rollout_proc7 to join...\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:52:49,063][00731] Batcher 0 profile tree view:\n","batching: 28.0726, releasing_batches: 0.0287\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:52:49,066][00731] InferenceWorker_p0-w0 profile tree view:\n","wait_policy: 0.0000\n","  wait_policy_total: 444.8646\n","update_model: 9.4437\n","  weight_update: 0.0044\n","one_step: 0.0301\n","  handle_policy_step: 631.3190\n","    deserialize: 16.0030, stack: 3.2450, obs_to_device_normalize: 128.7118, forward: 336.0799, send_messages: 30.0287\n","    prepare_outputs: 85.9776\n","      to_cpu: 50.5494\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:52:49,069][00731] Learner 0 profile tree view:\n","misc: 0.0060, prepare_batch: 13.8139\n","train: 75.0645\n","  epoch_init: 0.0059, minibatch_init: 0.0067, losses_postprocess: 0.6409, kl_divergence: 0.7861, after_optimizer: 34.1733\n","  calculate_losses: 27.1995\n","    losses_init: 0.0038, forward_head: 1.2997, bptt_initial: 18.2325, tail: 1.1846, advantages_returns: 0.2728, losses: 3.7784\n","    bptt: 2.1188\n","      bptt_forward_core: 2.0291\n","  update: 11.6173\n","    clip: 0.9459\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:52:49,071][00731] RolloutWorker_w0 profile tree view:\n","wait_for_trajectories: 0.3515, enqueue_policy_requests: 118.4487, env_step: 878.2726, overhead: 15.0806, complete_rollouts: 7.4899\n","save_policy_outputs: 20.3671\n","  split_output_tensors: 8.1967\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:52:49,072][00731] RolloutWorker_w7 profile tree view:\n","wait_for_trajectories: 0.3373, enqueue_policy_requests: 120.2483, env_step: 873.7391, overhead: 15.3858, complete_rollouts: 7.2561\n","save_policy_outputs: 20.6780\n","  split_output_tensors: 8.0919\u001b[0m\n","\u001b[36m[2024-07-24 09:52:49,074][00731] Loop Runner_EvtLoop terminating...\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:52:49,076][00731] Runner profile tree view:\n","main_loop: 1159.3226\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:52:49,077][00731] Collected {0: 4005888}, FPS: 3455.4\u001b[0m\n"]}],"source":["## Start the training, this should take around 15 minutes\n","register_vizdoom_components()\n","\n","# The scenario we train on today is health gathering\n","# other scenarios include \"doom_basic\", \"doom_two_colors_easy\", \"doom_dm\", \"doom_dwango5\", \"doom_my_way_home\", \"doom_deadly_corridor\", \"doom_defend_the_center\", \"doom_defend_the_line\"\n","env = \"doom_health_gathering_supreme\"\n","cfg = parse_vizdoom_cfg(\n","    argv=[f\"--env={env}\", \"--num_workers=8\", \"--num_envs_per_worker=4\", \"--train_for_env_steps=4000000\"]\n",")\n","\n","status = run_rl(cfg)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":88086,"status":"ok","timestamp":1721814862701,"user":{"displayName":"Vanipenta Naga Nithin Reddy","userId":"10356624584404995155"},"user_tz":-330},"id":"XisX-plVFPkq","outputId":"89aa4f3b-2c97-4bad-98ae-4ae8a2b81d2c"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n","\u001b[33m[2024-07-24 09:52:49,482][00731] Loading existing experiment configuration from /content/train_dir/default_experiment/config.json\u001b[0m\n","\u001b[36m[2024-07-24 09:52:49,484][00731] Overriding arg 'num_workers' with value 1 passed from command line\u001b[0m\n","\u001b[36m[2024-07-24 09:52:49,487][00731] Adding new argument 'no_render'=True that is not in the saved config file!\u001b[0m\n","\u001b[36m[2024-07-24 09:52:49,489][00731] Adding new argument 'save_video'=True that is not in the saved config file!\u001b[0m\n","\u001b[36m[2024-07-24 09:52:49,491][00731] Adding new argument 'video_frames'=1000000000.0 that is not in the saved config file!\u001b[0m\n","\u001b[36m[2024-07-24 09:52:49,493][00731] Adding new argument 'video_name'=None that is not in the saved config file!\u001b[0m\n","\u001b[36m[2024-07-24 09:52:49,496][00731] Adding new argument 'max_num_frames'=1000000000.0 that is not in the saved config file!\u001b[0m\n","\u001b[36m[2024-07-24 09:52:49,498][00731] Adding new argument 'max_num_episodes'=10 that is not in the saved config file!\u001b[0m\n","\u001b[36m[2024-07-24 09:52:49,501][00731] Adding new argument 'push_to_hub'=False that is not in the saved config file!\u001b[0m\n","\u001b[36m[2024-07-24 09:52:49,503][00731] Adding new argument 'hf_repository'=None that is not in the saved config file!\u001b[0m\n","\u001b[36m[2024-07-24 09:52:49,504][00731] Adding new argument 'policy_index'=0 that is not in the saved config file!\u001b[0m\n","\u001b[36m[2024-07-24 09:52:49,508][00731] Adding new argument 'eval_deterministic'=False that is not in the saved config file!\u001b[0m\n","\u001b[36m[2024-07-24 09:52:49,510][00731] Adding new argument 'train_script'=None that is not in the saved config file!\u001b[0m\n","\u001b[36m[2024-07-24 09:52:49,511][00731] Adding new argument 'enjoy_script'=None that is not in the saved config file!\u001b[0m\n","\u001b[36m[2024-07-24 09:52:49,512][00731] Using frameskip 1 and render_action_repeat=4 for evaluation\u001b[0m\n","\u001b[36m[2024-07-24 09:52:49,546][00731] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n","/usr/local/lib/python3.10/dist-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n","  logger.warn(\n","\u001b[36m[2024-07-24 09:52:49,550][00731] RunningMeanStd input shape: (3, 72, 128)\u001b[0m\n","\u001b[36m[2024-07-24 09:52:49,552][00731] RunningMeanStd input shape: (1,)\u001b[0m\n","\u001b[36m[2024-07-24 09:52:49,568][00731] ConvEncoder: input_channels=3\u001b[0m\n","\u001b[36m[2024-07-24 09:52:49,679][00731] Conv encoder output size: 512\u001b[0m\n","\u001b[36m[2024-07-24 09:52:49,680][00731] Policy head output size: 512\u001b[0m\n","\u001b[33m[2024-07-24 09:52:49,871][00731] Loading state from checkpoint /content/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:50,645][00731] Num frames 100...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:50,776][00731] Num frames 200...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:50,909][00731] Num frames 300...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:51,039][00731] Num frames 400...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:51,166][00731] Num frames 500...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:51,296][00731] Num frames 600...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:51,433][00731] Num frames 700...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:51,565][00731] Num frames 800...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:51,698][00731] Num frames 900...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:51,830][00731] Num frames 1000...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:51,958][00731] Num frames 1100...\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:52:52,121][00731] Avg episode rewards: #0: 23.840, true rewards: #0: 11.840\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:52:52,122][00731] Avg episode reward: 23.840, avg true_objective: 11.840\u001b[0m\n","\u001b[36m[2024-07-24 09:52:52,147][00731] Num frames 1200...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:52,275][00731] Num frames 1300...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:52,403][00731] Num frames 1400...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:52,540][00731] Num frames 1500...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:52,673][00731] Num frames 1600...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:52,807][00731] Num frames 1700...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:52,941][00731] Num frames 1800...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:53,108][00731] Num frames 1900...\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:52:53,314][00731] Avg episode rewards: #0: 18.900, true rewards: #0: 9.900\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:52:53,315][00731] Avg episode reward: 18.900, avg true_objective: 9.900\u001b[0m\n","\u001b[36m[2024-07-24 09:52:53,356][00731] Num frames 2000...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:53,552][00731] Num frames 2100...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:53,762][00731] Num frames 2200...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:53,954][00731] Num frames 2300...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:54,132][00731] Num frames 2400...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:54,311][00731] Num frames 2500...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:54,501][00731] Num frames 2600...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:54,709][00731] Num frames 2700...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:54,899][00731] Num frames 2800...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:55,089][00731] Num frames 2900...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:55,289][00731] Num frames 3000...\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:52:55,477][00731] Avg episode rewards: #0: 19.914, true rewards: #0: 10.247\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:52:55,478][00731] Avg episode reward: 19.914, avg true_objective: 10.247\u001b[0m\n","\u001b[36m[2024-07-24 09:52:55,514][00731] Num frames 3100...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:55,653][00731] Num frames 3200...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:55,786][00731] Num frames 3300...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:55,918][00731] Num frames 3400...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:56,053][00731] Num frames 3500...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:56,190][00731] Num frames 3600...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:56,322][00731] Num frames 3700...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:56,454][00731] Num frames 3800...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:56,584][00731] Num frames 3900...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:56,730][00731] Num frames 4000...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:56,863][00731] Num frames 4100...\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:52:56,937][00731] Avg episode rewards: #0: 20.780, true rewards: #0: 10.280\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:52:56,938][00731] Avg episode reward: 20.780, avg true_objective: 10.280\u001b[0m\n","\u001b[36m[2024-07-24 09:52:57,058][00731] Num frames 4200...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:57,190][00731] Num frames 4300...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:57,322][00731] Num frames 4400...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:57,452][00731] Num frames 4500...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:57,586][00731] Num frames 4600...\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:52:57,759][00731] Avg episode rewards: #0: 18.566, true rewards: #0: 9.366\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:52:57,761][00731] Avg episode reward: 18.566, avg true_objective: 9.366\u001b[0m\n","\u001b[36m[2024-07-24 09:52:57,790][00731] Num frames 4700...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:57,919][00731] Num frames 4800...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:58,048][00731] Num frames 4900...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:58,181][00731] Num frames 5000...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:58,314][00731] Num frames 5100...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:58,445][00731] Num frames 5200...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:58,578][00731] Num frames 5300...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:58,723][00731] Num frames 5400...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:58,852][00731] Num frames 5500...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:58,983][00731] Num frames 5600...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:59,112][00731] Num frames 5700...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:59,241][00731] Num frames 5800...\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:52:59,346][00731] Avg episode rewards: #0: 19.559, true rewards: #0: 9.725\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:52:59,348][00731] Avg episode reward: 19.559, avg true_objective: 9.725\u001b[0m\n","\u001b[36m[2024-07-24 09:52:59,434][00731] Num frames 5900...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:59,568][00731] Num frames 6000...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:59,705][00731] Num frames 6100...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:59,845][00731] Num frames 6200...\u001b[0m\n","\u001b[36m[2024-07-24 09:52:59,979][00731] Num frames 6300...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:00,107][00731] Num frames 6400...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:00,236][00731] Num frames 6500...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:00,366][00731] Num frames 6600...\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:53:00,557][00731] Avg episode rewards: #0: 19.713, true rewards: #0: 9.570\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:53:00,560][00731] Avg episode reward: 19.713, avg true_objective: 9.570\u001b[0m\n","\u001b[36m[2024-07-24 09:53:00,563][00731] Num frames 6700...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:00,699][00731] Num frames 6800...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:00,838][00731] Num frames 6900...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:00,966][00731] Num frames 7000...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:01,094][00731] Num frames 7100...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:01,223][00731] Num frames 7200...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:01,357][00731] Num frames 7300...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:01,486][00731] Num frames 7400...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:01,620][00731] Num frames 7500...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:01,758][00731] Num frames 7600...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:01,896][00731] Num frames 7700...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:02,030][00731] Num frames 7800...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:02,162][00731] Num frames 7900...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:02,294][00731] Num frames 8000...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:02,424][00731] Num frames 8100...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:02,553][00731] Num frames 8200...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:02,686][00731] Num frames 8300...\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:53:02,869][00731] Avg episode rewards: #0: 21.744, true rewards: #0: 10.494\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:53:02,870][00731] Avg episode reward: 21.744, avg true_objective: 10.494\u001b[0m\n","\u001b[36m[2024-07-24 09:53:02,882][00731] Num frames 8400...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:03,012][00731] Num frames 8500...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:03,144][00731] Num frames 8600...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:03,274][00731] Num frames 8700...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:03,404][00731] Num frames 8800...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:03,535][00731] Num frames 8900...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:03,669][00731] Num frames 9000...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:03,807][00731] Num frames 9100...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:03,951][00731] Num frames 9200...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:04,080][00731] Num frames 9300...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:04,208][00731] Num frames 9400...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:04,339][00731] Num frames 9500...\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:53:04,415][00731] Avg episode rewards: #0: 22.239, true rewards: #0: 10.572\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:53:04,417][00731] Avg episode reward: 22.239, avg true_objective: 10.572\u001b[0m\n","\u001b[36m[2024-07-24 09:53:04,527][00731] Num frames 9600...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:04,658][00731] Num frames 9700...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:04,793][00731] Num frames 9800...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:04,934][00731] Num frames 9900...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:05,065][00731] Num frames 10000...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:05,196][00731] Num frames 10100...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:05,329][00731] Num frames 10200...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:05,505][00731] Num frames 10300...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:05,706][00731] Num frames 10400...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:05,910][00731] Num frames 10500...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:06,091][00731] Num frames 10600...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:06,276][00731] Num frames 10700...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:06,456][00731] Num frames 10800...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:06,639][00731] Num frames 10900...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:06,835][00731] Num frames 11000...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:07,045][00731] Num frames 11100...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:07,241][00731] Num frames 11200...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:07,442][00731] Num frames 11300...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:07,617][00731] Num frames 11400...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:07,751][00731] Num frames 11500...\u001b[0m\n","\u001b[36m[2024-07-24 09:53:07,881][00731] Num frames 11600...\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:53:07,957][00731] Avg episode rewards: #0: 25.515, true rewards: #0: 11.615\u001b[0m\n","\u001b[37m\u001b[1m[2024-07-24 09:53:07,959][00731] Avg episode reward: 25.515, avg true_objective: 11.615\u001b[0m\n","\u001b[36m[2024-07-24 09:54:18,737][00731] Replay video saved to /content/train_dir/default_experiment/replay.mp4!\u001b[0m\n"]}],"source":["from sample_factory.enjoy import enjoy\n","\n","cfg = parse_vizdoom_cfg(\n","    argv=[f\"--env={env}\", \"--num_workers=1\", \"--save_video\", \"--no_render\", \"--max_num_episodes=10\"], evaluation=True\n",")\n","status = enjoy(cfg)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"elapsed":8,"status":"error","timestamp":1721815319393,"user":{"displayName":"Vanipenta Naga Nithin Reddy","userId":"10356624584404995155"},"user_tz":-330},"id":"FDSY57TUFTTI"},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/train_dir/default_experiment/replay.mp4'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-3-27737b1b7a1c\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 4\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 4\u001b[0;31m \u001b[0mmp4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/train_dir/default_experiment/replay.mp4\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdata_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"data:video/mp4;base64,\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb64encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmp4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m HTML(\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/train_dir/default_experiment/replay.mp4'"]}],"source":["from base64 import b64encode\n","from IPython.display import HTML\n","\n","mp4 = open(\"/content/train_dir/default_experiment/replay.mp4\", \"rb\").read()\n","data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n","HTML(\n","    \"\"\"\n","\u003cvideo width=640 controls\u003e\n","      \u003csource src=\"%s\" type=\"video/mp4\"\u003e\n","\u003c/video\u003e\n","\"\"\"\n","    % data_url\n",")"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":331},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1721815319392,"user":{"displayName":"Vanipenta Naga Nithin Reddy","userId":"10356624584404995155"},"user_tz":-330},"id":"OC6a6YBgLpH2","outputId":"880a580d-922c-4f5e-fda6-5b47e4d7ad40"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"89e19ca3abc541c18f6a88719d724132","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='\u003ccenter\u003e \u003cimg\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"]},"metadata":{},"output_type":"display_data"}],"source":["from huggingface_hub import notebook_login\n","notebook_login()\n","!git config --global credential.helper store"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"RPZCvgn0LztT"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'sample_factory'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-4-3893bb0aef3a\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 1\u003e\u001b[0;34m()\u001b[0m\n\u001b[0;32m----\u003e 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msample_factory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menjoy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0menjoy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mhf_username\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nithin04\"\u001b[0m  \u001b[0;31m# insert your HuggingFace username here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m cfg = parse_vizdoom_cfg(\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sample_factory'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}],"source":["from sample_factory.enjoy import enjoy\n","\n","hf_username = \"nithin04\"  # insert your HuggingFace username here\n","\n","cfg = parse_vizdoom_cfg(\n","    argv=[\n","        f\"--env={env}\",\n","        \"--num_workers=1\",\n","        \"--save_video\",\n","        \"--no_render\",\n","        \"--max_num_episodes=10\",\n","        \"--max_num_frames=100000\",\n","        \"--push_to_hub\",\n","        f\"--hf_repository={hf_username}/rl_course_vizdoom_health_gathering_supreme\",\n","    ],\n","    evaluation=True,\n",")\n","status = enjoy(cfg)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QYlYinaCL28W"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMfco0fhThL9/5hwEJiYsJ7","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0a47a7ebfb6243598f8c4abaa5a8aeb4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0fbb8bfe047345649173b95d2b09ee88":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"PasswordModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_54d8572b1f934ef8aa6b92dcb4f5598a","placeholder":"​","style":"IPY_MODEL_0a47a7ebfb6243598f8c4abaa5a8aeb4","value":""}},"54d8572b1f934ef8aa6b92dcb4f5598a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ac8cdfb32fa429baba6036bef2b738a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_666ea971746240f8a309f9ba56606dd4","placeholder":"​","style":"IPY_MODEL_c8c73dd03db747668ec3f927bc89ab6e","value":"\n\u003cb\u003ePro Tip:\u003c/b\u003e If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. \u003c/center\u003e"}},"655497347dc74485a46724cb1cbbd265":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ButtonModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_ae3ec4a862e544d0acf52fbc34093605","style":"IPY_MODEL_929df9cc10d14138962d18963fc15c91","tooltip":""}},"666ea971746240f8a309f9ba56606dd4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"680b579b3a5b43b49917514a9ebe753f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7f2ed85906ab481e95ce097b4b97a5c2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89e19ca3abc541c18f6a88719d724132":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_cbf941bc5d0c4ce480d3479a7dfbc6c5","IPY_MODEL_0fbb8bfe047345649173b95d2b09ee88","IPY_MODEL_aae548bdfb7049708bcd0f2555406477","IPY_MODEL_655497347dc74485a46724cb1cbbd265","IPY_MODEL_5ac8cdfb32fa429baba6036bef2b738a"],"layout":"IPY_MODEL_8e347481c80943509f07251b9d4c4064"}},"8e347481c80943509f07251b9d4c4064":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"929df9cc10d14138962d18963fc15c91":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ButtonStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"aae548bdfb7049708bcd0f2555406477":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"CheckboxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_7f2ed85906ab481e95ce097b4b97a5c2","style":"IPY_MODEL_ba339fe7b1da41e49e06fea2a6061d91","value":true}},"ae3ec4a862e544d0acf52fbc34093605":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b33b6f9aa2f44681b19bc8fc93780a2a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba339fe7b1da41e49e06fea2a6061d91":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c8c73dd03db747668ec3f927bc89ab6e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cbf941bc5d0c4ce480d3479a7dfbc6c5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b33b6f9aa2f44681b19bc8fc93780a2a","placeholder":"​","style":"IPY_MODEL_680b579b3a5b43b49917514a9ebe753f","value":"\u003ccenter\u003e \u003cimg\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'\u003e \u003cbr\u003e Copy a token from \u003ca\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\"\u003eyour Hugging Face\ntokens page\u003c/a\u003e and paste it below. \u003cbr\u003e Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. \u003c/center\u003e"}}}}},"nbformat":4,"nbformat_minor":0}